{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home App \n",
    "## Part IV- Predict the price of home using desired variables\n",
    "\n",
    "The data for this project comes from Kaggle  [House Sales in King County, USA](https://www.kaggle.com/architdxb/king-countyusa/data). The list of variables is shown below.\n",
    "\n",
    "The variables:\n",
    "\n",
    "* **id** : house identification number\n",
    "* **date** : date house was sold\n",
    "* **price** : target variable for prediction\n",
    "* **bedrooms** : number of bedrooms in the home\n",
    "* **bathrooms** : number of bathrooms in the home\n",
    "* **sqft_living** : square footage of the home\n",
    "* **sqft_lot** : square footage of the lot\n",
    "* **floors** : total floors in the home\n",
    "* **waterfront** : 1 for home with a waterfront view, 0 otherwise\n",
    "* **view** : has been viewed or not\n",
    "* **condition** : How good the condition is overall\n",
    "* **grade** : overall grade given to the housing unit, based on king county grading system\n",
    "* **sqft_above** : square footage of home apart from basement\n",
    "* **sqft_basement** : square footage of the basement\n",
    "* **yr_built** : year the home was built\n",
    "* **yr_renovated** : year the home was renovated\n",
    "* **zipcode** : zip code of the home\n",
    "* **lat** : latitude coordinate of the home\n",
    "* **long** : longitude coordinate of the home\n",
    "* **sqft_living15** : living room area in 2015\n",
    "* **sqft_lot15** : lot area in 2015\n",
    "\n",
    "For this part of the project, we will use the processed data from part III to create our ML model. \n",
    "\n",
    "For part IV of the project, we will: \n",
    "\n",
    "1. Read the processed data from Part III. \n",
    "2. Split the data into training/validation/test sets.  \n",
    "3. Train a model with the supervised ML method with lowest mean square error. \n",
    "4. Perform parameter tuning to acquire a model with better performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main dataset has 21969 samples with 8 features each.\n"
     ]
    }
   ],
   "source": [
    "# load necessary packages \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from time import time\n",
    "\n",
    "# pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "data_path = \"C:/Users/Yiruru/Documents/launchCode/data/processed_data.csv\"\n",
    "df= pd.read_csv(data_path)\n",
    "# Load and merge the housing data\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"Main dataset has {} samples with {} features each.\".format(*df.shape))\n",
    "except:\n",
    "    print(\"Dataset could not be loaded. Is the dataset missing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of the features have too many missing values, so we will only consider features with less than 10% of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>year</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>98178</td>\n",
       "      <td>1955</td>\n",
       "      <td>221900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>98125</td>\n",
       "      <td>1991</td>\n",
       "      <td>538000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  bedrooms  bathrooms  sqft_living  sqft_lot  zipcode  year  \\\n",
       "0  7129300520  3         1.00       1180         5650      98178    1955   \n",
       "1  6414100192  3         2.25       2570         7242      98125    1991   \n",
       "\n",
       "      price  \n",
       "0  221900.0  \n",
       "1  538000.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = df\n",
    "new_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.06717</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.478261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  bedrooms  bathrooms  sqft_living  sqft_lot   zipcode      year\n",
       "0  7129300520  0.090909  0.125      0.06717      0.003108  0.893939  0.478261"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler()\n",
    "numerical = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'year', 'zipcode']\n",
    "price_raw = new_data['price']\n",
    "features_raw = new_data.drop('price', axis = 1)\n",
    "features_raw[numerical] = scaler.fit_transform(new_data[numerical])\n",
    "\n",
    "# Show an example of a record with scaling applied\n",
    "display(features_raw.head(n = 1))\n",
    "features = pd.get_dummies(features_raw)\n",
    "price = new_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 15378 samples.\n",
      "Validation set has 3296 samples.\n",
      "Testing set has 3295 samples.\n",
      "Testing set is 15.0%.\n"
     ]
    }
   ],
   "source": [
    "# Train and Test \n",
    "X = new_data.ix[:, :-1]\n",
    "Y = new_data.ix[:, -1]\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state = 42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state = 42)\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(x_train.shape[0]))\n",
    "print(\"Validation set has {} samples.\".format(x_val.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(x_test.shape[0]))\n",
    "\n",
    "# confirm % of test set\n",
    "print(\"Testing set is {}%.\".format(round(1.*x_test.shape[0]/features.shape[0]*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3: Characteristics of Training, Validation, and Test Set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>validation Set</th>\n",
       "      <th>Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sample Size</th>\n",
       "      <td>15378</td>\n",
       "      <td>3296</td>\n",
       "      <td>3295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log error</th>\n",
       "      <td>540842.6695</td>\n",
       "      <td>529067.9600</td>\n",
       "      <td>534933.2155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Training Set validation Set     Test Set\n",
       "Sample Size  15378        3296           3295       \n",
       "log error    540842.6695  529067.9600    534933.2155"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data characteristics on the training set\n",
    "n_train = round(len(x_train), 0)\n",
    "n_val = round(len(x_val), 0)\n",
    "n_test = round(len(x_test),1)\n",
    "\n",
    "y_train_mean = y_train.mean()\n",
    "y_val_mean = y_val.mean()\n",
    "y_test_mean = y_test.mean()\n",
    "\n",
    "basic_df = pd.DataFrame([['{:.0f}'.format(n_train), '{:.0f}'.format(n_val), '{:.0f}'.format(n_test)], \n",
    "                         ['{:.4f}'.format(y_train_mean), '{:.4f}'.format(y_val_mean), '{:.4f}'.format(y_test_mean)]], \n",
    "                        index=['Sample Size', 'log error'], \n",
    "                        columns=['Training Set', 'validation Set', 'Test Set'])\n",
    "print('Table 3: Characteristics of Training, Validation, and Test Set')\n",
    "basic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGB Mean Square error  34438488683.429756\n",
      "GradientBoosting Mean Square error\"  34430681312.74244\n",
      "RandomForest Mean Square error\"  31821144159.05133\n",
      "DecisionTree Mean Square error\"  51008838755.838135\n",
      "AdaBoost Mean Square error\"  72296524448.7404\n",
      "Bagging Mean Square error\"  32954825236.770092\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#create classifiers\n",
    "xgbrg= XGBRegressor(objective=\"reg:linear\", random_state=100) \n",
    "gbrg = GradientBoostingRegressor(random_state=100) \n",
    "rfrg = RandomForestRegressor(random_state=100) \n",
    "dtrg = DecisionTreeRegressor(random_state=100) \n",
    "abrg = AdaBoostRegressor(random_state=100) \n",
    "brg = BaggingRegressor(random_state=100)\n",
    "\n",
    "# train classifiers\n",
    "xgbrg.fit(x_train, y_train, eval_metric = 'rmse', verbose = False, eval_set = [(x_test, y_test)])\n",
    "gbrg.fit(x_train, y_train)\n",
    "rfrg.fit(x_train, y_train)\n",
    "dtrg.fit(x_train, y_train)\n",
    "abrg.fit(x_train, y_train)\n",
    "brg.fit(x_train, y_train)\n",
    "\n",
    "# obtain predictions\n",
    "xgbpreds = xgbrg.predict(x_val)\n",
    "gbpreds = gbrg.predict(x_val)\n",
    "rfpreds = rfrg.predict(x_val)\n",
    "dtpreds = dtrg.predict(x_val)\n",
    "abpreds = abrg.predict(x_val)\n",
    "bpreds = brg.predict(x_val)\n",
    "\n",
    "print('\\nXGB Mean Square error ', mean_squared_error(y_val,xgbpreds))\n",
    "print('GradientBoosting Mean Square error\" ', mean_squared_error(y_val,gbpreds))\n",
    "print('RandomForest Mean Square error\" ', mean_squared_error(y_val,rfpreds))\n",
    "print('DecisionTree Mean Square error\" ', mean_squared_error(y_val,dtpreds))\n",
    "print('AdaBoost Mean Square error\" ', mean_squared_error(y_val,abpreds))\n",
    "print('Bagging Mean Square error\" ', mean_squared_error(y_val,bpreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that gradient boosting performs the best, but we will actually choose XGB model for its known superiority in terms of training speed. We will now perform model tuning on XGB model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer, precision_score, recall_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "scoring_function = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "target = 'price'\n",
    "predictors = new_data.columns[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=early_stopping_rounds, verbose_eval= True)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['price'],eval_metric='rmse')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "     \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Mean Square error : %.4g\" % metrics.mean_squared_error(dtrain['price'].values, dtrain_predictions))\n",
    "    print(\"r2 Score : %.4g\" % metrics.r2_score(dtrain['price'].values, dtrain_predictions))\n",
    "    return dtrain_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:602546+1090.83\ttest-rmse:602940+8134.71\n",
      "[1]\ttrain-rmse:552709+1040.61\ttest-rmse:553839+8766.35\n",
      "[2]\ttrain-rmse:508203+960.483\ttest-rmse:509390+7562.28\n",
      "[3]\ttrain-rmse:470507+2155.98\ttest-rmse:472249+6514.04\n",
      "[4]\ttrain-rmse:435323+2732.44\ttest-rmse:437718+5981.07\n",
      "[5]\ttrain-rmse:404077+2644.35\ttest-rmse:407123+6029.11\n",
      "[6]\ttrain-rmse:376198+2206.88\ttest-rmse:379751+5363.95\n",
      "[7]\ttrain-rmse:352384+2614.44\ttest-rmse:356687+6351.09\n",
      "[8]\ttrain-rmse:331441+2757.37\ttest-rmse:336569+5774.12\n",
      "[9]\ttrain-rmse:313352+2683.93\ttest-rmse:319445+6439.34\n",
      "[10]\ttrain-rmse:296593+2688.99\ttest-rmse:303311+5989.64\n",
      "[11]\ttrain-rmse:282759+3593.02\ttest-rmse:290041+5739.72\n",
      "[12]\ttrain-rmse:270718+3584.95\ttest-rmse:278829+5623.19\n",
      "[13]\ttrain-rmse:259907+3416.24\ttest-rmse:268748+5465.93\n",
      "[14]\ttrain-rmse:250433+3408.83\ttest-rmse:260136+5304.65\n",
      "[15]\ttrain-rmse:242206+3089.67\ttest-rmse:252316+5145.43\n",
      "[16]\ttrain-rmse:234948+2925.41\ttest-rmse:245609+4868.51\n",
      "[17]\ttrain-rmse:228615+2178.57\ttest-rmse:240040+4063.32\n",
      "[18]\ttrain-rmse:223111+2215.9\ttest-rmse:235456+4359.03\n",
      "[19]\ttrain-rmse:218068+2078.89\ttest-rmse:231035+4528.45\n",
      "[20]\ttrain-rmse:213602+1976.53\ttest-rmse:227302+4325\n",
      "[21]\ttrain-rmse:209713+1717.39\ttest-rmse:224119+4143.91\n",
      "[22]\ttrain-rmse:206295+1667.06\ttest-rmse:221317+3767.18\n",
      "[23]\ttrain-rmse:203415+1590.82\ttest-rmse:218819+3629.48\n",
      "[24]\ttrain-rmse:200715+1640.45\ttest-rmse:216620+3236.16\n",
      "[25]\ttrain-rmse:198275+1439.31\ttest-rmse:214652+3300.43\n",
      "[26]\ttrain-rmse:195719+1304.09\ttest-rmse:212634+2984.08\n",
      "[27]\ttrain-rmse:194004+1285.69\ttest-rmse:211478+2833.92\n",
      "[28]\ttrain-rmse:192135+1657.6\ttest-rmse:210134+2411.12\n",
      "[29]\ttrain-rmse:190091+1324.66\ttest-rmse:208762+2515.46\n",
      "[30]\ttrain-rmse:188322+1388.37\ttest-rmse:207342+2271.75\n",
      "[31]\ttrain-rmse:186734+1356.85\ttest-rmse:205957+2690.44\n",
      "[32]\ttrain-rmse:185661+1340.75\ttest-rmse:205206+2498.61\n",
      "[33]\ttrain-rmse:183962+1722.5\ttest-rmse:204095+2966.15\n",
      "[34]\ttrain-rmse:182871+1797.46\ttest-rmse:203424+2653.99\n",
      "[35]\ttrain-rmse:180913+1801.13\ttest-rmse:201992+2200.23\n",
      "[36]\ttrain-rmse:179014+1777.79\ttest-rmse:200435+1956.3\n",
      "[37]\ttrain-rmse:178084+1821.71\ttest-rmse:200011+1801.23\n",
      "[38]\ttrain-rmse:177254+2078.62\ttest-rmse:199421+1687.32\n",
      "[39]\ttrain-rmse:175928+1646.63\ttest-rmse:198312+1608.38\n",
      "[40]\ttrain-rmse:175009+1359.11\ttest-rmse:197640+1399.48\n",
      "[41]\ttrain-rmse:174251+1711.9\ttest-rmse:197113+1475.33\n",
      "[42]\ttrain-rmse:173549+1903.15\ttest-rmse:196736+1189.12\n",
      "[43]\ttrain-rmse:172897+1991.08\ttest-rmse:196352+1118.62\n",
      "[44]\ttrain-rmse:171906+2424.16\ttest-rmse:195744+901.778\n",
      "[45]\ttrain-rmse:170708+1978.41\ttest-rmse:194896+1321.49\n",
      "[46]\ttrain-rmse:169636+1501.59\ttest-rmse:194059+1662.45\n",
      "[47]\ttrain-rmse:168381+1118.96\ttest-rmse:192849+2401.47\n",
      "[48]\ttrain-rmse:167631+990.762\ttest-rmse:192558+2333.49\n",
      "[49]\ttrain-rmse:166905+1445.87\ttest-rmse:192171+2324.46\n",
      "[50]\ttrain-rmse:166301+1589.92\ttest-rmse:191768+2143.8\n",
      "[51]\ttrain-rmse:165122+1379.81\ttest-rmse:190892+2140.46\n",
      "[52]\ttrain-rmse:164281+1655.79\ttest-rmse:190262+1602.16\n",
      "[53]\ttrain-rmse:163568+1998.32\ttest-rmse:190036+1349.26\n",
      "[54]\ttrain-rmse:162566+1987.53\ttest-rmse:189393+1517.18\n",
      "[55]\ttrain-rmse:161860+1853.57\ttest-rmse:188975+1744.16\n",
      "[56]\ttrain-rmse:160997+1882.05\ttest-rmse:188366+1910.05\n",
      "[57]\ttrain-rmse:160588+2011.57\ttest-rmse:188216+1916.2\n",
      "[58]\ttrain-rmse:159868+1888.34\ttest-rmse:187626+2010.68\n",
      "[59]\ttrain-rmse:159323+2105.9\ttest-rmse:187319+2067.43\n",
      "[60]\ttrain-rmse:158582+2311.88\ttest-rmse:186935+1965.08\n",
      "[61]\ttrain-rmse:158214+2413.08\ttest-rmse:186814+2129.74\n",
      "[62]\ttrain-rmse:157626+2501.19\ttest-rmse:186406+2083.74\n",
      "[63]\ttrain-rmse:157191+2520.16\ttest-rmse:186203+2404.25\n",
      "[64]\ttrain-rmse:156294+2560.6\ttest-rmse:185585+2618.09\n",
      "[65]\ttrain-rmse:155121+2370.54\ttest-rmse:184641+2587.62\n",
      "[66]\ttrain-rmse:154567+2523.52\ttest-rmse:184440+2644.89\n",
      "[67]\ttrain-rmse:153787+2357.31\ttest-rmse:183947+2676.84\n",
      "[68]\ttrain-rmse:153130+2364.81\ttest-rmse:183656+2696.22\n",
      "[69]\ttrain-rmse:152389+2243.68\ttest-rmse:183296+2681.41\n",
      "[70]\ttrain-rmse:151837+2186.42\ttest-rmse:183049+2891.42\n",
      "[71]\ttrain-rmse:151333+2205.09\ttest-rmse:182774+3060.1\n",
      "[72]\ttrain-rmse:150985+2249\ttest-rmse:182647+3216.21\n",
      "[73]\ttrain-rmse:150602+2261.72\ttest-rmse:182375+3353.64\n",
      "[74]\ttrain-rmse:150142+2345.35\ttest-rmse:182201+3431.16\n",
      "[75]\ttrain-rmse:149504+2479.43\ttest-rmse:181770+3602.06\n",
      "[76]\ttrain-rmse:149045+2504.56\ttest-rmse:181514+3644.27\n",
      "[77]\ttrain-rmse:148604+2396.92\ttest-rmse:181281+3741.56\n",
      "[78]\ttrain-rmse:148215+2547.72\ttest-rmse:181193+3699.56\n",
      "[79]\ttrain-rmse:147838+2325.5\ttest-rmse:181038+3806.64\n",
      "[80]\ttrain-rmse:147255+2022.04\ttest-rmse:180706+4026.46\n",
      "[81]\ttrain-rmse:146893+1987.7\ttest-rmse:180689+4097.65\n",
      "[82]\ttrain-rmse:146461+2052.16\ttest-rmse:180625+4054.7\n",
      "[83]\ttrain-rmse:145852+1796.78\ttest-rmse:180266+4019.67\n",
      "[84]\ttrain-rmse:145484+1865.95\ttest-rmse:180079+4096.26\n",
      "[85]\ttrain-rmse:145263+1897.65\ttest-rmse:180114+4083.11\n",
      "[86]\ttrain-rmse:144644+1687.75\ttest-rmse:179910+4164.13\n",
      "[87]\ttrain-rmse:144281+1693.23\ttest-rmse:179737+4331.58\n",
      "[88]\ttrain-rmse:143891+1855.32\ttest-rmse:179658+4295.34\n",
      "[89]\ttrain-rmse:143488+1815.19\ttest-rmse:179591+4181.44\n",
      "[90]\ttrain-rmse:143151+1662.54\ttest-rmse:179432+4061.02\n",
      "[91]\ttrain-rmse:142656+1525.9\ttest-rmse:179096+4173.77\n",
      "[92]\ttrain-rmse:142267+1433.95\ttest-rmse:178897+4031.22\n",
      "[93]\ttrain-rmse:141735+1284.69\ttest-rmse:178784+4005.57\n",
      "[94]\ttrain-rmse:141350+1405.1\ttest-rmse:178550+4009.3\n",
      "[95]\ttrain-rmse:140926+1496.68\ttest-rmse:178383+3930.73\n",
      "[96]\ttrain-rmse:140638+1348.9\ttest-rmse:178180+4007.84\n",
      "[97]\ttrain-rmse:140125+1175.68\ttest-rmse:177941+4111.15\n",
      "[98]\ttrain-rmse:139777+1196.37\ttest-rmse:177924+4059.95\n",
      "[99]\ttrain-rmse:139541+1234.87\ttest-rmse:177948+4120.48\n",
      "[100]\ttrain-rmse:139170+1231.61\ttest-rmse:177796+4097.4\n",
      "[101]\ttrain-rmse:138862+1244.6\ttest-rmse:177643+4062.72\n",
      "[102]\ttrain-rmse:138576+1229.24\ttest-rmse:177564+4042.53\n",
      "[103]\ttrain-rmse:138350+1156.43\ttest-rmse:177561+4050.22\n",
      "[104]\ttrain-rmse:137995+1139.55\ttest-rmse:177405+4226.58\n",
      "[105]\ttrain-rmse:137662+1222.17\ttest-rmse:177291+4160.23\n",
      "[106]\ttrain-rmse:137315+1174.28\ttest-rmse:177105+4150.72\n",
      "[107]\ttrain-rmse:136950+1041.4\ttest-rmse:176968+4176.81\n",
      "[108]\ttrain-rmse:136664+939.227\ttest-rmse:176877+4263.57\n",
      "[109]\ttrain-rmse:136357+847.552\ttest-rmse:176881+4324.47\n",
      "[110]\ttrain-rmse:136053+879.978\ttest-rmse:176790+4397.83\n",
      "[111]\ttrain-rmse:135762+896.359\ttest-rmse:176687+4410.86\n",
      "[112]\ttrain-rmse:135423+952.634\ttest-rmse:176578+4359.67\n",
      "[113]\ttrain-rmse:135153+985.865\ttest-rmse:176419+4198.44\n",
      "[114]\ttrain-rmse:134818+914.326\ttest-rmse:176171+4309.48\n",
      "[115]\ttrain-rmse:134443+927.492\ttest-rmse:176062+4269.85\n",
      "[116]\ttrain-rmse:134158+921.261\ttest-rmse:175972+4215.37\n",
      "[117]\ttrain-rmse:133846+914.653\ttest-rmse:175845+4270.45\n",
      "[118]\ttrain-rmse:133534+777.079\ttest-rmse:175686+4238.02\n",
      "[119]\ttrain-rmse:133264+822.325\ttest-rmse:175580+4229.35\n",
      "[120]\ttrain-rmse:132993+780.177\ttest-rmse:175585+4176.72\n",
      "[121]\ttrain-rmse:132655+838.715\ttest-rmse:175603+4087.5\n",
      "[122]\ttrain-rmse:132415+890.634\ttest-rmse:175453+4057.87\n",
      "[123]\ttrain-rmse:132039+803.632\ttest-rmse:175292+4162.95\n",
      "[124]\ttrain-rmse:131765+833.377\ttest-rmse:175170+4075.84\n",
      "[125]\ttrain-rmse:131568+894.306\ttest-rmse:175088+4050.72\n",
      "[126]\ttrain-rmse:131326+925.135\ttest-rmse:175126+4029.94\n",
      "[127]\ttrain-rmse:130941+918.549\ttest-rmse:174890+4026.66\n",
      "[128]\ttrain-rmse:130661+978.534\ttest-rmse:174805+3956.55\n",
      "[129]\ttrain-rmse:130419+871.054\ttest-rmse:174650+4006.56\n",
      "[130]\ttrain-rmse:130130+732.97\ttest-rmse:174613+4039.3\n",
      "[131]\ttrain-rmse:129872+665.903\ttest-rmse:174590+4063.76\n",
      "[132]\ttrain-rmse:129663+701.131\ttest-rmse:174440+4033.42\n",
      "[133]\ttrain-rmse:129398+731.813\ttest-rmse:174374+4117.68\n",
      "[134]\ttrain-rmse:129196+768.681\ttest-rmse:174301+4148.43\n",
      "[135]\ttrain-rmse:128959+794.327\ttest-rmse:174294+4140.08\n",
      "[136]\ttrain-rmse:128724+790.001\ttest-rmse:174243+4140.17\n",
      "[137]\ttrain-rmse:128531+791.106\ttest-rmse:174206+4034.61\n",
      "[138]\ttrain-rmse:128263+742.711\ttest-rmse:174141+4096.84\n",
      "[139]\ttrain-rmse:128017+771.7\ttest-rmse:174158+4042.51\n",
      "[140]\ttrain-rmse:127684+836.845\ttest-rmse:174043+3958.68\n",
      "[141]\ttrain-rmse:127484+946.351\ttest-rmse:173932+3949.04\n",
      "[142]\ttrain-rmse:127338+926.874\ttest-rmse:173937+3963.35\n",
      "[143]\ttrain-rmse:127103+810.75\ttest-rmse:173848+4033.82\n",
      "[144]\ttrain-rmse:126734+704.862\ttest-rmse:173706+4164.5\n",
      "[145]\ttrain-rmse:126519+737.478\ttest-rmse:173750+4129.14\n",
      "[146]\ttrain-rmse:126198+664.949\ttest-rmse:173650+4170.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147]\ttrain-rmse:126006+595.067\ttest-rmse:173664+4170.36\n",
      "[148]\ttrain-rmse:125772+669.83\ttest-rmse:173603+4163.98\n",
      "[149]\ttrain-rmse:125522+708.156\ttest-rmse:173539+4098.23\n",
      "[150]\ttrain-rmse:125268+677.911\ttest-rmse:173398+4146.11\n",
      "[151]\ttrain-rmse:125068+724.476\ttest-rmse:173330+4080.83\n",
      "[152]\ttrain-rmse:124835+734.31\ttest-rmse:173339+4044.83\n",
      "[153]\ttrain-rmse:124639+687.707\ttest-rmse:173302+4084.01\n",
      "[154]\ttrain-rmse:124390+632.515\ttest-rmse:173227+4055\n",
      "[155]\ttrain-rmse:124183+729.216\ttest-rmse:173182+4052.38\n",
      "[156]\ttrain-rmse:123990+648.55\ttest-rmse:173082+4131.13\n",
      "[157]\ttrain-rmse:123747+756.618\ttest-rmse:173032+4101.47\n",
      "[158]\ttrain-rmse:123553+766.776\ttest-rmse:172946+4113.51\n",
      "[159]\ttrain-rmse:123273+754.921\ttest-rmse:172878+4090.85\n",
      "[160]\ttrain-rmse:123050+739.276\ttest-rmse:172747+4107.5\n",
      "[161]\ttrain-rmse:122847+709.69\ttest-rmse:172806+4227.22\n",
      "[162]\ttrain-rmse:122633+769.419\ttest-rmse:172794+4171.73\n",
      "[163]\ttrain-rmse:122429+785.27\ttest-rmse:172813+4198.64\n",
      "[164]\ttrain-rmse:122278+817.196\ttest-rmse:172843+4237.91\n",
      "[165]\ttrain-rmse:122093+792.854\ttest-rmse:172826+4313.02\n",
      "[166]\ttrain-rmse:121940+773.816\ttest-rmse:172758+4387.58\n",
      "[167]\ttrain-rmse:121701+737.068\ttest-rmse:172615+4384.84\n",
      "[168]\ttrain-rmse:121462+747.226\ttest-rmse:172487+4470.7\n",
      "[169]\ttrain-rmse:121315+722.381\ttest-rmse:172447+4406.33\n",
      "[170]\ttrain-rmse:121070+722.843\ttest-rmse:172401+4336.5\n",
      "[171]\ttrain-rmse:120903+632.782\ttest-rmse:172449+4323.89\n",
      "[172]\ttrain-rmse:120719+672.609\ttest-rmse:172407+4373.62\n",
      "[173]\ttrain-rmse:120544+708.376\ttest-rmse:172409+4325.76\n",
      "[174]\ttrain-rmse:120384+731.877\ttest-rmse:172384+4340.89\n",
      "[175]\ttrain-rmse:120235+770.801\ttest-rmse:172373+4364.03\n",
      "[176]\ttrain-rmse:120010+783.758\ttest-rmse:172276+4372.87\n",
      "[177]\ttrain-rmse:119876+803.602\ttest-rmse:172309+4378.66\n",
      "[178]\ttrain-rmse:119590+855.825\ttest-rmse:172187+4417.5\n",
      "[179]\ttrain-rmse:119431+819.826\ttest-rmse:172201+4397.83\n",
      "[180]\ttrain-rmse:119254+787.014\ttest-rmse:172137+4403.29\n",
      "[181]\ttrain-rmse:119096+791.706\ttest-rmse:172100+4426.06\n",
      "[182]\ttrain-rmse:118944+818.903\ttest-rmse:172059+4398.42\n",
      "[183]\ttrain-rmse:118750+777.955\ttest-rmse:171960+4416.64\n",
      "[184]\ttrain-rmse:118547+795.002\ttest-rmse:171905+4491.66\n",
      "[185]\ttrain-rmse:118401+826.223\ttest-rmse:171904+4466.41\n",
      "[186]\ttrain-rmse:118255+830.691\ttest-rmse:171897+4499.11\n",
      "[187]\ttrain-rmse:118114+866.835\ttest-rmse:171880+4519.64\n",
      "[188]\ttrain-rmse:117971+825.458\ttest-rmse:171801+4574.87\n",
      "[189]\ttrain-rmse:117814+835.863\ttest-rmse:171811+4618.26\n",
      "[190]\ttrain-rmse:117602+875.299\ttest-rmse:171808+4607.68\n",
      "[191]\ttrain-rmse:117446+884.029\ttest-rmse:171780+4596.98\n",
      "[192]\ttrain-rmse:117257+874.195\ttest-rmse:171763+4485.99\n",
      "[193]\ttrain-rmse:117024+884.972\ttest-rmse:171798+4342.46\n",
      "[194]\ttrain-rmse:116875+909.362\ttest-rmse:171816+4290.32\n",
      "[195]\ttrain-rmse:116749+925.952\ttest-rmse:171845+4250.68\n",
      "[196]\ttrain-rmse:116607+931.914\ttest-rmse:171809+4239.23\n",
      "[197]\ttrain-rmse:116453+960.125\ttest-rmse:171808+4241.75\n",
      "[198]\ttrain-rmse:116289+937.561\ttest-rmse:171798+4218.99\n",
      "[199]\ttrain-rmse:116130+923.634\ttest-rmse:171780+4228.73\n",
      "[200]\ttrain-rmse:115949+910.657\ttest-rmse:171777+4233.9\n",
      "[201]\ttrain-rmse:115824+901.161\ttest-rmse:171760+4224.79\n",
      "[202]\ttrain-rmse:115648+815.506\ttest-rmse:171689+4307.53\n",
      "[203]\ttrain-rmse:115559+812.85\ttest-rmse:171694+4297.09\n",
      "[204]\ttrain-rmse:115415+801.238\ttest-rmse:171709+4272.58\n",
      "[205]\ttrain-rmse:115285+850.252\ttest-rmse:171691+4267.88\n",
      "[206]\ttrain-rmse:115124+907.654\ttest-rmse:171664+4295.99\n",
      "[207]\ttrain-rmse:114919+984.682\ttest-rmse:171568+4296.18\n",
      "[208]\ttrain-rmse:114787+951.989\ttest-rmse:171550+4271.94\n",
      "[209]\ttrain-rmse:114628+980.064\ttest-rmse:171487+4250.13\n",
      "[210]\ttrain-rmse:114472+1001.5\ttest-rmse:171407+4218.95\n",
      "[211]\ttrain-rmse:114279+938.592\ttest-rmse:171330+4249.39\n",
      "[212]\ttrain-rmse:114051+989.334\ttest-rmse:171173+4182.15\n",
      "[213]\ttrain-rmse:113839+990.406\ttest-rmse:171142+4208.74\n",
      "[214]\ttrain-rmse:113707+996.386\ttest-rmse:171131+4205.74\n",
      "[215]\ttrain-rmse:113639+987.135\ttest-rmse:171098+4179.68\n",
      "[216]\ttrain-rmse:113488+946.997\ttest-rmse:171047+4142.93\n",
      "[217]\ttrain-rmse:113379+945.367\ttest-rmse:171036+4143.24\n",
      "[218]\ttrain-rmse:113165+930.899\ttest-rmse:170954+4172.55\n",
      "[219]\ttrain-rmse:113004+909.779\ttest-rmse:170925+4152.5\n",
      "[220]\ttrain-rmse:112856+910.378\ttest-rmse:170854+4150.7\n",
      "[221]\ttrain-rmse:112753+927.742\ttest-rmse:170842+4137.01\n",
      "[222]\ttrain-rmse:112630+931.913\ttest-rmse:170843+4138.96\n",
      "[223]\ttrain-rmse:112486+915.838\ttest-rmse:170879+4141.53\n",
      "[224]\ttrain-rmse:112362+919.502\ttest-rmse:170881+4105.49\n",
      "[225]\ttrain-rmse:112218+932.743\ttest-rmse:170818+4062.31\n",
      "[226]\ttrain-rmse:112045+954.311\ttest-rmse:170840+4029.46\n",
      "[227]\ttrain-rmse:111918+975.896\ttest-rmse:170868+4058.32\n",
      "[228]\ttrain-rmse:111747+993.075\ttest-rmse:170875+4128.02\n",
      "[229]\ttrain-rmse:111580+1012.19\ttest-rmse:170739+4167.1\n",
      "[230]\ttrain-rmse:111440+1047.25\ttest-rmse:170703+4166.84\n",
      "[231]\ttrain-rmse:111253+1038.23\ttest-rmse:170638+4157.56\n",
      "[232]\ttrain-rmse:111060+1071.16\ttest-rmse:170645+4089.74\n",
      "[233]\ttrain-rmse:110891+1053.15\ttest-rmse:170604+4033.93\n",
      "[234]\ttrain-rmse:110782+1047.47\ttest-rmse:170605+4021.48\n",
      "[235]\ttrain-rmse:110642+1041.16\ttest-rmse:170595+4032.7\n",
      "[236]\ttrain-rmse:110486+1113.49\ttest-rmse:170570+4043.12\n",
      "[237]\ttrain-rmse:110406+1123.35\ttest-rmse:170585+4068.14\n",
      "[238]\ttrain-rmse:110249+1121.38\ttest-rmse:170533+4082.83\n",
      "[239]\ttrain-rmse:110140+1114.36\ttest-rmse:170539+4063.67\n",
      "[240]\ttrain-rmse:110004+1091.98\ttest-rmse:170504+4144.98\n",
      "[241]\ttrain-rmse:109890+1112.33\ttest-rmse:170458+4133.96\n",
      "[242]\ttrain-rmse:109776+1125.84\ttest-rmse:170431+4116.76\n",
      "[243]\ttrain-rmse:109637+1146.03\ttest-rmse:170385+4157.4\n",
      "[244]\ttrain-rmse:109552+1144.34\ttest-rmse:170401+4091.35\n",
      "[245]\ttrain-rmse:109412+1098.89\ttest-rmse:170386+4076.99\n",
      "[246]\ttrain-rmse:109282+1107.78\ttest-rmse:170361+4101.02\n",
      "[247]\ttrain-rmse:109131+1072.09\ttest-rmse:170327+4075.94\n",
      "[248]\ttrain-rmse:108990+1080.25\ttest-rmse:170354+4071.37\n",
      "[249]\ttrain-rmse:108816+1022.15\ttest-rmse:170324+4059.06\n",
      "[250]\ttrain-rmse:108677+1047.9\ttest-rmse:170324+4042.86\n",
      "[251]\ttrain-rmse:108533+1058.44\ttest-rmse:170287+4073.72\n",
      "[252]\ttrain-rmse:108376+1059.15\ttest-rmse:170256+4077.99\n",
      "[253]\ttrain-rmse:108234+1030.55\ttest-rmse:170306+4117.72\n",
      "[254]\ttrain-rmse:108110+1039.45\ttest-rmse:170300+4116.41\n",
      "[255]\ttrain-rmse:108004+1027.14\ttest-rmse:170303+4113.77\n",
      "[256]\ttrain-rmse:107890+1004.9\ttest-rmse:170271+4128.07\n",
      "[257]\ttrain-rmse:107792+994.628\ttest-rmse:170276+4136.41\n",
      "[258]\ttrain-rmse:107677+981.018\ttest-rmse:170251+4176.24\n",
      "[259]\ttrain-rmse:107545+1006.59\ttest-rmse:170216+4193.46\n",
      "[260]\ttrain-rmse:107416+1034.79\ttest-rmse:170212+4214.22\n",
      "[261]\ttrain-rmse:107312+1060.99\ttest-rmse:170193+4207.2\n",
      "[262]\ttrain-rmse:107230+1082.41\ttest-rmse:170146+4176.67\n",
      "[263]\ttrain-rmse:107101+1086.79\ttest-rmse:170149+4171.49\n",
      "[264]\ttrain-rmse:106939+1098.25\ttest-rmse:170124+4178.31\n",
      "[265]\ttrain-rmse:106798+1100.31\ttest-rmse:170094+4200.06\n",
      "[266]\ttrain-rmse:106714+1114.29\ttest-rmse:170096+4206.36\n",
      "[267]\ttrain-rmse:106612+1120.75\ttest-rmse:170104+4220.58\n",
      "[268]\ttrain-rmse:106495+1110.07\ttest-rmse:170074+4175.56\n",
      "[269]\ttrain-rmse:106386+1090.65\ttest-rmse:170083+4175.8\n",
      "[270]\ttrain-rmse:106264+1092.52\ttest-rmse:170073+4212.71\n",
      "[271]\ttrain-rmse:106132+1120.02\ttest-rmse:170050+4273.56\n",
      "[272]\ttrain-rmse:106012+1118.37\ttest-rmse:170050+4227.5\n",
      "[273]\ttrain-rmse:105892+1156.65\ttest-rmse:170058+4208.47\n",
      "[274]\ttrain-rmse:105737+1200.52\ttest-rmse:170009+4201.19\n",
      "[275]\ttrain-rmse:105617+1201.66\ttest-rmse:170032+4210.49\n",
      "[276]\ttrain-rmse:105504+1179.27\ttest-rmse:170065+4211.97\n",
      "[277]\ttrain-rmse:105405+1159.67\ttest-rmse:170058+4217.74\n",
      "[278]\ttrain-rmse:105309+1161.69\ttest-rmse:170020+4177.12\n",
      "[279]\ttrain-rmse:105209+1183.51\ttest-rmse:170010+4147.95\n",
      "[280]\ttrain-rmse:105042+1187.92\ttest-rmse:170014+4139.54\n",
      "[281]\ttrain-rmse:104971+1179.82\ttest-rmse:170019+4157.89\n",
      "[282]\ttrain-rmse:104856+1144.23\ttest-rmse:170034+4170.53\n",
      "[283]\ttrain-rmse:104777+1141.55\ttest-rmse:170042+4174.33\n",
      "[284]\ttrain-rmse:104658+1116.31\ttest-rmse:170028+4164.59\n",
      "[285]\ttrain-rmse:104549+1094.3\ttest-rmse:169979+4102.44\n",
      "[286]\ttrain-rmse:104419+1108\ttest-rmse:169978+4092.17\n",
      "[287]\ttrain-rmse:104280+1079.01\ttest-rmse:169943+4095.93\n",
      "[288]\ttrain-rmse:104193+1081.32\ttest-rmse:169972+4130.44\n",
      "[289]\ttrain-rmse:104069+1066.73\ttest-rmse:169964+4153.21\n",
      "[290]\ttrain-rmse:103960+1024.22\ttest-rmse:169959+4147.97\n",
      "[291]\ttrain-rmse:103835+1051.38\ttest-rmse:170004+4131.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[292]\ttrain-rmse:103725+1072.7\ttest-rmse:169981+4119.64\n",
      "[293]\ttrain-rmse:103599+1082.69\ttest-rmse:169972+4138\n",
      "[294]\ttrain-rmse:103509+1093.94\ttest-rmse:169944+4109.15\n",
      "[295]\ttrain-rmse:103364+1045.98\ttest-rmse:169951+4141.28\n",
      "[296]\ttrain-rmse:103266+1063.55\ttest-rmse:169913+4161.79\n",
      "[297]\ttrain-rmse:103129+1038.61\ttest-rmse:169895+4143.54\n",
      "[298]\ttrain-rmse:103014+1018.35\ttest-rmse:169856+4136.24\n",
      "[299]\ttrain-rmse:102935+1023.05\ttest-rmse:169853+4152.37\n",
      "[300]\ttrain-rmse:102850+982.603\ttest-rmse:169865+4162.31\n",
      "[301]\ttrain-rmse:102743+969.643\ttest-rmse:169842+4165.45\n",
      "[302]\ttrain-rmse:102647+983.409\ttest-rmse:169869+4183.19\n",
      "[303]\ttrain-rmse:102503+965.558\ttest-rmse:169870+4190.26\n",
      "[304]\ttrain-rmse:102375+976.692\ttest-rmse:169885+4223.78\n",
      "[305]\ttrain-rmse:102275+1006.52\ttest-rmse:169914+4246.09\n",
      "[306]\ttrain-rmse:102159+1000.69\ttest-rmse:169899+4202.24\n",
      "[307]\ttrain-rmse:102039+1007.71\ttest-rmse:169941+4151.98\n",
      "[308]\ttrain-rmse:101918+1024.42\ttest-rmse:169934+4172.72\n",
      "[309]\ttrain-rmse:101836+1031.97\ttest-rmse:169918+4186.71\n",
      "[310]\ttrain-rmse:101723+1044.06\ttest-rmse:169909+4187.26\n",
      "[311]\ttrain-rmse:101598+1014.79\ttest-rmse:169871+4199.1\n",
      "[312]\ttrain-rmse:101489+1010.79\ttest-rmse:169884+4215.4\n",
      "[313]\ttrain-rmse:101411+999.73\ttest-rmse:169908+4194.33\n",
      "[314]\ttrain-rmse:101317+970.592\ttest-rmse:169886+4174.08\n",
      "[315]\ttrain-rmse:101200+945.537\ttest-rmse:169900+4180.91\n",
      "[316]\ttrain-rmse:101106+947.448\ttest-rmse:169889+4145.55\n",
      "[317]\ttrain-rmse:101012+941.782\ttest-rmse:169876+4156.91\n",
      "[318]\ttrain-rmse:100871+943.852\ttest-rmse:169857+4168.01\n",
      "[319]\ttrain-rmse:100754+929.784\ttest-rmse:169832+4163.5\n",
      "[320]\ttrain-rmse:100661+907.953\ttest-rmse:169802+4159.3\n",
      "[321]\ttrain-rmse:100550+878.879\ttest-rmse:169762+4174.92\n",
      "[322]\ttrain-rmse:100447+885.621\ttest-rmse:169773+4171.8\n",
      "[323]\ttrain-rmse:100312+891.411\ttest-rmse:169755+4165.57\n",
      "[324]\ttrain-rmse:100214+906.049\ttest-rmse:169748+4166.87\n",
      "[325]\ttrain-rmse:100110+869.719\ttest-rmse:169789+4182.14\n",
      "[326]\ttrain-rmse:100004+872.084\ttest-rmse:169773+4164.98\n",
      "[327]\ttrain-rmse:99908.1+852.087\ttest-rmse:169805+4166.15\n",
      "[328]\ttrain-rmse:99826.8+830.881\ttest-rmse:169831+4167.43\n",
      "[329]\ttrain-rmse:99732.2+808.378\ttest-rmse:169833+4169.26\n",
      "[330]\ttrain-rmse:99643.2+808.931\ttest-rmse:169798+4179.61\n",
      "[331]\ttrain-rmse:99580.3+813.716\ttest-rmse:169804+4195.3\n",
      "[332]\ttrain-rmse:99493.5+846.419\ttest-rmse:169798+4178.02\n",
      "[333]\ttrain-rmse:99390.5+851.181\ttest-rmse:169824+4189.04\n",
      "[334]\ttrain-rmse:99283.2+840.216\ttest-rmse:169809+4193.68\n",
      "[335]\ttrain-rmse:99183.8+854.663\ttest-rmse:169849+4184.02\n",
      "[336]\ttrain-rmse:99050+851.785\ttest-rmse:169853+4218.39\n",
      "[337]\ttrain-rmse:98946.7+851.149\ttest-rmse:169845+4219.41\n",
      "[338]\ttrain-rmse:98857+840.375\ttest-rmse:169825+4232.23\n",
      "[339]\ttrain-rmse:98765.9+861.318\ttest-rmse:169797+4231.69\n",
      "[340]\ttrain-rmse:98700.4+860.11\ttest-rmse:169789+4209.72\n",
      "[341]\ttrain-rmse:98605.7+895.517\ttest-rmse:169791+4236.78\n",
      "[342]\ttrain-rmse:98507.3+891.58\ttest-rmse:169790+4234.2\n",
      "[343]\ttrain-rmse:98418+901.409\ttest-rmse:169806+4252.47\n",
      "[344]\ttrain-rmse:98337.4+894.419\ttest-rmse:169804+4256.83\n",
      "[345]\ttrain-rmse:98223.4+912.859\ttest-rmse:169807+4277.76\n",
      "[346]\ttrain-rmse:98151.5+921.468\ttest-rmse:169759+4209.38\n",
      "[347]\ttrain-rmse:98059.7+923.839\ttest-rmse:169785+4192.21\n",
      "[348]\ttrain-rmse:97970.8+945.433\ttest-rmse:169733+4192.08\n",
      "[349]\ttrain-rmse:97864.3+952.134\ttest-rmse:169736+4183.86\n",
      "[350]\ttrain-rmse:97754.1+905.28\ttest-rmse:169755+4182.12\n",
      "[351]\ttrain-rmse:97642.8+885.851\ttest-rmse:169715+4150.46\n",
      "[352]\ttrain-rmse:97515.3+897.074\ttest-rmse:169698+4152.98\n",
      "[353]\ttrain-rmse:97432.8+907.979\ttest-rmse:169669+4149.32\n",
      "[354]\ttrain-rmse:97345.6+882.243\ttest-rmse:169661+4160.24\n",
      "[355]\ttrain-rmse:97257.3+882.376\ttest-rmse:169638+4138.47\n",
      "[356]\ttrain-rmse:97191.6+878.268\ttest-rmse:169638+4150.27\n",
      "[357]\ttrain-rmse:97093.9+851.119\ttest-rmse:169579+4147.86\n",
      "[358]\ttrain-rmse:97020.2+865.236\ttest-rmse:169605+4127.23\n",
      "[359]\ttrain-rmse:96927.9+878.565\ttest-rmse:169608+4127.1\n",
      "[360]\ttrain-rmse:96841.5+855.893\ttest-rmse:169587+4164.99\n",
      "[361]\ttrain-rmse:96754.7+835.697\ttest-rmse:169608+4158.43\n",
      "[362]\ttrain-rmse:96659.3+818.342\ttest-rmse:169652+4154.66\n",
      "[363]\ttrain-rmse:96581.2+802.682\ttest-rmse:169627+4154.26\n",
      "[364]\ttrain-rmse:96475.3+807.3\ttest-rmse:169606+4154.56\n",
      "[365]\ttrain-rmse:96387.7+818.026\ttest-rmse:169629+4147.06\n",
      "[366]\ttrain-rmse:96291.5+803.345\ttest-rmse:169632+4162.1\n",
      "[367]\ttrain-rmse:96198.6+784.284\ttest-rmse:169647+4196.88\n",
      "[368]\ttrain-rmse:96129+797.833\ttest-rmse:169657+4185\n",
      "[369]\ttrain-rmse:96062.6+795.689\ttest-rmse:169661+4200.7\n",
      "[370]\ttrain-rmse:95978+785.437\ttest-rmse:169698+4200.76\n",
      "[371]\ttrain-rmse:95895+793.717\ttest-rmse:169707+4176.49\n",
      "[372]\ttrain-rmse:95829.3+787.305\ttest-rmse:169710+4156.46\n",
      "[373]\ttrain-rmse:95742.2+773.878\ttest-rmse:169717+4177.32\n",
      "[374]\ttrain-rmse:95660.9+757.929\ttest-rmse:169754+4173.76\n",
      "[375]\ttrain-rmse:95580.9+766.649\ttest-rmse:169758+4165.01\n",
      "[376]\ttrain-rmse:95489+777.862\ttest-rmse:169747+4163.83\n",
      "[377]\ttrain-rmse:95413.4+776.297\ttest-rmse:169773+4142.98\n",
      "[378]\ttrain-rmse:95339.5+776.471\ttest-rmse:169751+4161.13\n",
      "[379]\ttrain-rmse:95254.5+786.259\ttest-rmse:169763+4138.44\n",
      "[380]\ttrain-rmse:95184.6+786.826\ttest-rmse:169772+4132.27\n",
      "[381]\ttrain-rmse:95088+801.937\ttest-rmse:169767+4145.28\n",
      "[382]\ttrain-rmse:95022.4+805.799\ttest-rmse:169770+4149.42\n",
      "[383]\ttrain-rmse:94952.3+781.99\ttest-rmse:169767+4151.3\n",
      "[384]\ttrain-rmse:94867.5+796.043\ttest-rmse:169726+4190.75\n",
      "[385]\ttrain-rmse:94817.6+791.3\ttest-rmse:169728+4177.35\n",
      "[386]\ttrain-rmse:94693.8+791.798\ttest-rmse:169734+4179.8\n",
      "[387]\ttrain-rmse:94614.8+768.676\ttest-rmse:169728+4134.63\n",
      "[388]\ttrain-rmse:94556.3+757.926\ttest-rmse:169767+4132.34\n",
      "[389]\ttrain-rmse:94493.6+761.038\ttest-rmse:169787+4127.9\n",
      "[390]\ttrain-rmse:94402.1+772.6\ttest-rmse:169823+4159.02\n",
      "[391]\ttrain-rmse:94319.8+763.189\ttest-rmse:169820+4172.18\n",
      "[392]\ttrain-rmse:94240.6+749.935\ttest-rmse:169817+4160.01\n",
      "[393]\ttrain-rmse:94151+773.619\ttest-rmse:169806+4146.04\n",
      "[394]\ttrain-rmse:94061.4+751.051\ttest-rmse:169795+4149.3\n",
      "[395]\ttrain-rmse:93973.9+755.603\ttest-rmse:169817+4140.97\n",
      "[396]\ttrain-rmse:93913.6+766.424\ttest-rmse:169796+4127.42\n",
      "[397]\ttrain-rmse:93823.9+749.533\ttest-rmse:169795+4133.65\n",
      "[398]\ttrain-rmse:93735.6+770.345\ttest-rmse:169782+4121.8\n",
      "[399]\ttrain-rmse:93632.1+763.024\ttest-rmse:169771+4074.18\n",
      "[400]\ttrain-rmse:93557.2+787.007\ttest-rmse:169759+4114.5\n",
      "[401]\ttrain-rmse:93470.5+748.363\ttest-rmse:169771+4118.95\n",
      "[402]\ttrain-rmse:93368.8+766.756\ttest-rmse:169747+4120.66\n",
      "[403]\ttrain-rmse:93315.9+764.188\ttest-rmse:169766+4131.8\n",
      "[404]\ttrain-rmse:93243.7+756.116\ttest-rmse:169805+4114.45\n",
      "[405]\ttrain-rmse:93165.2+721.231\ttest-rmse:169799+4128.27\n",
      "[406]\ttrain-rmse:93128.3+720.61\ttest-rmse:169850+4127.59\n",
      "\n",
      "Model Report\n",
      "Mean Square error : 1.06e+10\n",
      "r2 Score : 0.9241\n"
     ]
    }
   ],
   "source": [
    "#combine x_train and y_train for model tuning \n",
    "df_train = pd.concat([x_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# get a benchmark performance with our initial values\n",
    "xgb1 = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=100)\n",
    "\n",
    "m1pred = modelfit(xgb1, df_train, predictors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.78003, std: 0.01353, params: {'max_depth': 4, 'min_child_weight': 1},\n",
       "  mean: 0.77870, std: 0.01224, params: {'max_depth': 4, 'min_child_weight': 2},\n",
       "  mean: 0.77794, std: 0.00640, params: {'max_depth': 4, 'min_child_weight': 3},\n",
       "  mean: 0.77640, std: 0.01268, params: {'max_depth': 4, 'min_child_weight': 4},\n",
       "  mean: 0.78298, std: 0.01345, params: {'max_depth': 5, 'min_child_weight': 1},\n",
       "  mean: 0.78221, std: 0.00999, params: {'max_depth': 5, 'min_child_weight': 2},\n",
       "  mean: 0.77895, std: 0.01059, params: {'max_depth': 5, 'min_child_weight': 3},\n",
       "  mean: 0.78253, std: 0.00856, params: {'max_depth': 5, 'min_child_weight': 4},\n",
       "  mean: 0.78290, std: 0.01649, params: {'max_depth': 6, 'min_child_weight': 1},\n",
       "  mean: 0.78206, std: 0.01454, params: {'max_depth': 6, 'min_child_weight': 2},\n",
       "  mean: 0.77995, std: 0.01256, params: {'max_depth': 6, 'min_child_weight': 3},\n",
       "  mean: 0.77836, std: 0.00559, params: {'max_depth': 6, 'min_child_weight': 4},\n",
       "  mean: 0.77958, std: 0.01978, params: {'max_depth': 7, 'min_child_weight': 1},\n",
       "  mean: 0.77777, std: 0.01605, params: {'max_depth': 7, 'min_child_weight': 2},\n",
       "  mean: 0.77606, std: 0.01202, params: {'max_depth': 7, 'min_child_weight': 3},\n",
       "  mean: 0.78254, std: 0.01093, params: {'max_depth': 7, 'min_child_weight': 4}],\n",
       " {'max_depth': 5, 'min_child_weight': 1},\n",
       " 0.7829789098686998)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':[4, 5, 6, 7],\n",
    " 'min_child_weight':[1, 2, 3, 4]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=406, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=100), \n",
    " param_grid = param_test1,n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(df_train[predictors],df_train[target])\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value for max_depth and min_child_weight are 5 and 1, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.78106, std: 0.01349, params: {'gamma': 0.0},\n",
       "  mean: 0.78106, std: 0.01349, params: {'gamma': 0.1},\n",
       "  mean: 0.78106, std: 0.01349, params: {'gamma': 0.2},\n",
       "  mean: 0.78106, std: 0.01349, params: {'gamma': 0.3},\n",
       "  mean: 0.78106, std: 0.01349, params: {'gamma': 0.4},\n",
       "  mean: 0.78106, std: 0.01349, params: {'gamma': 0.5},\n",
       "  mean: 0.78106, std: 0.01349, params: {'gamma': 0.6},\n",
       "  mean: 0.78106, std: 0.01349, params: {'gamma': 0.7},\n",
       "  mean: 0.78106, std: 0.01349, params: {'gamma': 0.8},\n",
       "  mean: 0.78106, std: 0.01349, params: {'gamma': 0.9},\n",
       "  mean: 0.78106, std: 0.01349, params: {'gamma': 1.0}],\n",
       " {'gamma': 0.0},\n",
       " 0.7810647551413255)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {\n",
    " 'gamma':[i/10.0 for i in range(0,11)]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators=406, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=100), param_grid = param_test2,n_jobs=4,iid=False, cv=5)\n",
    "gsearch2.fit(df_train[predictors],df_train[target])\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value for gamma is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.77729, std: 0.01728, params: {'colsample_bytree': 0.5, 'subsample': 0.6},\n",
       "  mean: 0.77802, std: 0.01705, params: {'colsample_bytree': 0.5, 'subsample': 0.7},\n",
       "  mean: 0.77833, std: 0.01582, params: {'colsample_bytree': 0.5, 'subsample': 0.8},\n",
       "  mean: 0.78196, std: 0.01686, params: {'colsample_bytree': 0.5, 'subsample': 0.9},\n",
       "  mean: 0.77863, std: 0.02213, params: {'colsample_bytree': 0.5, 'subsample': 1.0},\n",
       "  mean: 0.77729, std: 0.01728, params: {'colsample_bytree': 0.6, 'subsample': 0.6},\n",
       "  mean: 0.77802, std: 0.01705, params: {'colsample_bytree': 0.6, 'subsample': 0.7},\n",
       "  mean: 0.77833, std: 0.01582, params: {'colsample_bytree': 0.6, 'subsample': 0.8},\n",
       "  mean: 0.78196, std: 0.01686, params: {'colsample_bytree': 0.6, 'subsample': 0.9},\n",
       "  mean: 0.77863, std: 0.02213, params: {'colsample_bytree': 0.6, 'subsample': 1.0},\n",
       "  mean: 0.78146, std: 0.01327, params: {'colsample_bytree': 0.7, 'subsample': 0.6},\n",
       "  mean: 0.78312, std: 0.01408, params: {'colsample_bytree': 0.7, 'subsample': 0.7},\n",
       "  mean: 0.78298, std: 0.01345, params: {'colsample_bytree': 0.7, 'subsample': 0.8},\n",
       "  mean: 0.78446, std: 0.01565, params: {'colsample_bytree': 0.7, 'subsample': 0.9},\n",
       "  mean: 0.78334, std: 0.01320, params: {'colsample_bytree': 0.7, 'subsample': 1.0},\n",
       "  mean: 0.78146, std: 0.01327, params: {'colsample_bytree': 0.8, 'subsample': 0.6},\n",
       "  mean: 0.78312, std: 0.01408, params: {'colsample_bytree': 0.8, 'subsample': 0.7},\n",
       "  mean: 0.78298, std: 0.01345, params: {'colsample_bytree': 0.8, 'subsample': 0.8},\n",
       "  mean: 0.78446, std: 0.01565, params: {'colsample_bytree': 0.8, 'subsample': 0.9},\n",
       "  mean: 0.78334, std: 0.01320, params: {'colsample_bytree': 0.8, 'subsample': 1.0},\n",
       "  mean: 0.77820, std: 0.00980, params: {'colsample_bytree': 0.9, 'subsample': 0.6},\n",
       "  mean: 0.78484, std: 0.01373, params: {'colsample_bytree': 0.9, 'subsample': 0.7},\n",
       "  mean: 0.78250, std: 0.01143, params: {'colsample_bytree': 0.9, 'subsample': 0.8},\n",
       "  mean: 0.78436, std: 0.01425, params: {'colsample_bytree': 0.9, 'subsample': 0.9},\n",
       "  mean: 0.78269, std: 0.01480, params: {'colsample_bytree': 0.9, 'subsample': 1.0},\n",
       "  mean: 0.77694, std: 0.01186, params: {'colsample_bytree': 1.0, 'subsample': 0.6},\n",
       "  mean: 0.78190, std: 0.00960, params: {'colsample_bytree': 1.0, 'subsample': 0.7},\n",
       "  mean: 0.77972, std: 0.01204, params: {'colsample_bytree': 1.0, 'subsample': 0.8},\n",
       "  mean: 0.78196, std: 0.01861, params: {'colsample_bytree': 1.0, 'subsample': 0.9},\n",
       "  mean: 0.78465, std: 0.01888, params: {'colsample_bytree': 1.0, 'subsample': 1.0}],\n",
       " {'colsample_bytree': 0.9, 'subsample': 0.7},\n",
       " 0.7848415900536286)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'subsample':[i/10.0 for i in range(6,11)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(5,11)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, n_estimators= 406, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=100), \n",
    " param_grid = param_test3, n_jobs=4,iid=False, cv=5)\n",
    "gsearch3.fit(df_train[predictors],df_train[target])\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimum values for colsample_bytree and subsample are 0.9 and 0.7, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.78484, std: 0.01373, params: {'reg_alpha': 1e-05},\n",
       "  mean: 0.78484, std: 0.01373, params: {'reg_alpha': 0.01},\n",
       "  mean: 0.78484, std: 0.01373, params: {'reg_alpha': 0.1},\n",
       "  mean: 0.78484, std: 0.01373, params: {'reg_alpha': 1},\n",
       "  mean: 0.78486, std: 0.01373, params: {'reg_alpha': 10},\n",
       "  mean: 0.78463, std: 0.01368, params: {'reg_alpha': 50},\n",
       "  mean: 0.78592, std: 0.01366, params: {'reg_alpha': 100}],\n",
       " {'reg_alpha': 100},\n",
       " 0.7859156375943824)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 10, 50, 100]\n",
    "}\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = XGBRegressor(learning_rate =0.1, n_estimators= 406, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.7, colsample_bytree=0.9,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=100), \n",
    " param_grid = param_test4, n_jobs=4,iid=False, cv=5)\n",
    "gsearch4.fit(df_train[predictors],df_train[target])\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.78447, std: 0.01390, params: {'reg_alpha': 60},\n",
       "  mean: 0.78544, std: 0.01273, params: {'reg_alpha': 70},\n",
       "  mean: 0.78544, std: 0.01273, params: {'reg_alpha': 80},\n",
       "  mean: 0.78580, std: 0.01330, params: {'reg_alpha': 90},\n",
       "  mean: 0.78592, std: 0.01366, params: {'reg_alpha': 100},\n",
       "  mean: 0.78559, std: 0.01386, params: {'reg_alpha': 110},\n",
       "  mean: 0.78503, std: 0.01257, params: {'reg_alpha': 120},\n",
       "  mean: 0.78484, std: 0.01327, params: {'reg_alpha': 130}],\n",
       " {'reg_alpha': 100},\n",
       " 0.7859156375943824)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4b = {\n",
    " 'reg_alpha':[60, 70, 80, 90, 100, 110, 120, 130]\n",
    "}\n",
    "gsearch4b = GridSearchCV(estimator = XGBRegressor(learning_rate =0.1, n_estimators= 406, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.7, colsample_bytree=0.9,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=100), \n",
    " param_grid = param_test4b, n_jobs=4,iid=False, cv=5)\n",
    "gsearch4b.fit(df_train[predictors],df_train[target])\n",
    "gsearch4b.grid_scores_, gsearch4b.best_params_, gsearch4b.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimum value for reg_alpha is 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the performance of this tuned model with regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:602334+1114.87\ttest-rmse:603075+8580.98\n",
      "[1]\ttrain-rmse:551849+916.909\ttest-rmse:552850+7810.92\n",
      "[2]\ttrain-rmse:507197+850.422\ttest-rmse:508695+7294.42\n",
      "[3]\ttrain-rmse:468356+1573.89\ttest-rmse:470780+7346.68\n",
      "[4]\ttrain-rmse:433092+1412.38\ttest-rmse:436074+7465.96\n",
      "[5]\ttrain-rmse:402322+2294.77\ttest-rmse:405812+7251.33\n",
      "[6]\ttrain-rmse:374600+2128.02\ttest-rmse:378746+6745.1\n",
      "[7]\ttrain-rmse:350810+2980.57\ttest-rmse:355288+6862.58\n",
      "[8]\ttrain-rmse:329632+2756.56\ttest-rmse:335120+6743.31\n",
      "[9]\ttrain-rmse:311208+2458.73\ttest-rmse:317684+6752.69\n",
      "[10]\ttrain-rmse:294710+2391.33\ttest-rmse:301683+6198.34\n",
      "[11]\ttrain-rmse:280710+2571.77\ttest-rmse:288323+5936.22\n",
      "[12]\ttrain-rmse:268511+2227.47\ttest-rmse:276922+5424.58\n",
      "[13]\ttrain-rmse:257055+2389.82\ttest-rmse:266179+5660.41\n",
      "[14]\ttrain-rmse:247708+2237.88\ttest-rmse:257528+5447.78\n",
      "[15]\ttrain-rmse:239451+1899.8\ttest-rmse:249975+5156.38\n",
      "[16]\ttrain-rmse:232264+1885.32\ttest-rmse:243321+4983.32\n",
      "[17]\ttrain-rmse:226109+1694.09\ttest-rmse:237654+4344.84\n",
      "[18]\ttrain-rmse:220449+1719.04\ttest-rmse:232729+4079.13\n",
      "[19]\ttrain-rmse:215663+1407.38\ttest-rmse:228800+3518.15\n",
      "[20]\ttrain-rmse:211379+1323.77\ttest-rmse:225166+3294.55\n",
      "[21]\ttrain-rmse:207812+1276.19\ttest-rmse:222065+3030.98\n",
      "[22]\ttrain-rmse:204490+1077.16\ttest-rmse:219698+2289.56\n",
      "[23]\ttrain-rmse:201490+1406.51\ttest-rmse:217252+2454.65\n",
      "[24]\ttrain-rmse:199026+1622.2\ttest-rmse:215378+2291.24\n",
      "[25]\ttrain-rmse:196934+1406.7\ttest-rmse:213491+2359.52\n",
      "[26]\ttrain-rmse:195021+1082.66\ttest-rmse:211991+2068.93\n",
      "[27]\ttrain-rmse:192808+1317.26\ttest-rmse:210451+2090.67\n",
      "[28]\ttrain-rmse:191016+1332.36\ttest-rmse:208900+1808.15\n",
      "[29]\ttrain-rmse:189436+1223.85\ttest-rmse:207932+1673.97\n",
      "[30]\ttrain-rmse:187614+1022.16\ttest-rmse:206414+1856.98\n",
      "[31]\ttrain-rmse:185943+965.296\ttest-rmse:204997+2247.34\n",
      "[32]\ttrain-rmse:184598+1020.12\ttest-rmse:204075+2019.57\n",
      "[33]\ttrain-rmse:183199+1259.03\ttest-rmse:203278+1813.36\n",
      "[34]\ttrain-rmse:182014+1482.39\ttest-rmse:202238+1593.39\n",
      "[35]\ttrain-rmse:180723+1641.83\ttest-rmse:201447+1561.46\n",
      "[36]\ttrain-rmse:179694+1637.97\ttest-rmse:200639+1616.29\n",
      "[37]\ttrain-rmse:178560+1579.32\ttest-rmse:199979+1406.01\n",
      "[38]\ttrain-rmse:177219+1645.74\ttest-rmse:199026+1295.62\n",
      "[39]\ttrain-rmse:176361+1687.56\ttest-rmse:198378+1321.21\n",
      "[40]\ttrain-rmse:175367+1611.45\ttest-rmse:197780+1567.48\n",
      "[41]\ttrain-rmse:174839+1689.4\ttest-rmse:197672+1660.63\n",
      "[42]\ttrain-rmse:174128+1782.33\ttest-rmse:197289+1674.05\n",
      "[43]\ttrain-rmse:173546+1836.05\ttest-rmse:197004+1598.11\n",
      "[44]\ttrain-rmse:172441+2384.92\ttest-rmse:196173+1199.25\n",
      "[45]\ttrain-rmse:171393+2168.27\ttest-rmse:195446+1367.52\n",
      "[46]\ttrain-rmse:170442+2023.56\ttest-rmse:194727+1694.63\n",
      "[47]\ttrain-rmse:169094+2096.54\ttest-rmse:193614+2442.41\n",
      "[48]\ttrain-rmse:168382+2348.48\ttest-rmse:193032+1982.94\n",
      "[49]\ttrain-rmse:167250+2837.25\ttest-rmse:192235+1681.46\n",
      "[50]\ttrain-rmse:166055+2996.34\ttest-rmse:191305+1854.72\n",
      "[51]\ttrain-rmse:164781+2512.48\ttest-rmse:190279+1898.71\n",
      "[52]\ttrain-rmse:163961+1928.97\ttest-rmse:189879+1794\n",
      "[53]\ttrain-rmse:163041+2103.81\ttest-rmse:189361+1965.31\n",
      "[54]\ttrain-rmse:162495+2163\ttest-rmse:189065+2234.96\n",
      "[55]\ttrain-rmse:161643+1878.6\ttest-rmse:188560+2551.73\n",
      "[56]\ttrain-rmse:160996+2047.85\ttest-rmse:188148+2310.12\n",
      "[57]\ttrain-rmse:160262+2233.13\ttest-rmse:187800+2509.12\n",
      "[58]\ttrain-rmse:159040+2241.64\ttest-rmse:186823+2339.56\n",
      "[59]\ttrain-rmse:158653+2220.12\ttest-rmse:186613+2289.84\n",
      "[60]\ttrain-rmse:158002+2067.57\ttest-rmse:186160+2598.23\n",
      "[61]\ttrain-rmse:157498+1778.19\ttest-rmse:186023+2741.65\n",
      "[62]\ttrain-rmse:156893+1876.67\ttest-rmse:185729+2904.4\n",
      "[63]\ttrain-rmse:156107+2140.22\ttest-rmse:185241+3148.93\n",
      "[64]\ttrain-rmse:155733+2072.11\ttest-rmse:184924+3194.4\n",
      "[65]\ttrain-rmse:154814+1906.67\ttest-rmse:184364+3051.3\n",
      "[66]\ttrain-rmse:154330+1918.51\ttest-rmse:184076+3095.54\n",
      "[67]\ttrain-rmse:153627+1856.18\ttest-rmse:183775+2940.37\n",
      "[68]\ttrain-rmse:152951+1925.21\ttest-rmse:183475+2877.05\n",
      "[69]\ttrain-rmse:152364+2035.43\ttest-rmse:183138+2845.55\n",
      "[70]\ttrain-rmse:151617+1778\ttest-rmse:182832+2938.63\n",
      "[71]\ttrain-rmse:151035+1906.84\ttest-rmse:182387+2968.07\n",
      "[72]\ttrain-rmse:150489+1693.47\ttest-rmse:182150+3257.62\n",
      "[73]\ttrain-rmse:150028+1719.06\ttest-rmse:182112+3320.62\n",
      "[74]\ttrain-rmse:149567+1705.49\ttest-rmse:181891+3437.21\n",
      "[75]\ttrain-rmse:149098+1780.86\ttest-rmse:181483+3294.74\n",
      "[76]\ttrain-rmse:148585+1826.18\ttest-rmse:181123+3114.69\n",
      "[77]\ttrain-rmse:148175+1916.24\ttest-rmse:180989+3234.67\n",
      "[78]\ttrain-rmse:147563+1578.02\ttest-rmse:180653+3481.29\n",
      "[79]\ttrain-rmse:147153+1480.92\ttest-rmse:180530+3555.88\n",
      "[80]\ttrain-rmse:146819+1378.1\ttest-rmse:180434+3771.82\n",
      "[81]\ttrain-rmse:146497+1443.31\ttest-rmse:180375+3691.2\n",
      "[82]\ttrain-rmse:146053+1590.41\ttest-rmse:180104+3611.93\n",
      "[83]\ttrain-rmse:145690+1631.31\ttest-rmse:180004+3578.65\n",
      "[84]\ttrain-rmse:145157+1254.45\ttest-rmse:179799+3790.02\n",
      "[85]\ttrain-rmse:144536+1394.75\ttest-rmse:179438+3736.61\n",
      "[86]\ttrain-rmse:144144+1242.6\ttest-rmse:179271+3760.39\n",
      "[87]\ttrain-rmse:143722+1215.11\ttest-rmse:179245+3812.78\n",
      "[88]\ttrain-rmse:143152+1154.62\ttest-rmse:178923+3797.14\n",
      "[89]\ttrain-rmse:142745+1294.73\ttest-rmse:178936+3671.26\n",
      "[90]\ttrain-rmse:142242+1374.67\ttest-rmse:178809+3401.69\n",
      "[91]\ttrain-rmse:141948+1342.04\ttest-rmse:178732+3394.34\n",
      "[92]\ttrain-rmse:141464+1230.89\ttest-rmse:178420+3407.23\n",
      "[93]\ttrain-rmse:141031+1147.66\ttest-rmse:178335+3514.38\n",
      "[94]\ttrain-rmse:140674+971.113\ttest-rmse:178171+3659.99\n",
      "[95]\ttrain-rmse:140117+890.845\ttest-rmse:177780+3858.41\n",
      "[96]\ttrain-rmse:139643+863.38\ttest-rmse:177571+3920.95\n",
      "[97]\ttrain-rmse:139308+856.134\ttest-rmse:177602+3924.89\n",
      "[98]\ttrain-rmse:138993+893.331\ttest-rmse:177551+3862.7\n",
      "[99]\ttrain-rmse:138582+830.174\ttest-rmse:177347+3895.63\n",
      "[100]\ttrain-rmse:138204+937.926\ttest-rmse:177116+3872.42\n",
      "[101]\ttrain-rmse:137946+961.275\ttest-rmse:177137+3804.28\n",
      "[102]\ttrain-rmse:137551+1002.36\ttest-rmse:176946+3793.12\n",
      "[103]\ttrain-rmse:137247+945.076\ttest-rmse:176804+3851.05\n",
      "[104]\ttrain-rmse:136827+1017.64\ttest-rmse:176639+3721.04\n",
      "[105]\ttrain-rmse:136560+1089.8\ttest-rmse:176580+3797.98\n",
      "[106]\ttrain-rmse:136273+1150.11\ttest-rmse:176501+3751.87\n",
      "[107]\ttrain-rmse:135891+1070.46\ttest-rmse:176349+3841.55\n",
      "[108]\ttrain-rmse:135563+976.466\ttest-rmse:176247+3870.95\n",
      "[109]\ttrain-rmse:135273+1014.57\ttest-rmse:176153+3792.58\n",
      "[110]\ttrain-rmse:134925+1066.94\ttest-rmse:176191+3679.3\n",
      "[111]\ttrain-rmse:134554+1133.71\ttest-rmse:176021+3615.55\n",
      "[112]\ttrain-rmse:134258+1167.02\ttest-rmse:175909+3559.73\n",
      "[113]\ttrain-rmse:133928+1037.59\ttest-rmse:175763+3595.27\n",
      "[114]\ttrain-rmse:133649+1108.28\ttest-rmse:175584+3645.4\n",
      "[115]\ttrain-rmse:133424+1134.12\ttest-rmse:175560+3738.55\n",
      "[116]\ttrain-rmse:133160+1170.19\ttest-rmse:175560+3740.49\n",
      "[117]\ttrain-rmse:132825+1292.87\ttest-rmse:175398+3613.84\n",
      "[118]\ttrain-rmse:132456+1335.34\ttest-rmse:175183+3505.9\n",
      "[119]\ttrain-rmse:132119+1503.74\ttest-rmse:175018+3432.77\n",
      "[120]\ttrain-rmse:131761+1376.81\ttest-rmse:174983+3577.65\n",
      "[121]\ttrain-rmse:131450+1425.09\ttest-rmse:174971+3579.96\n",
      "[122]\ttrain-rmse:131272+1419.19\ttest-rmse:174963+3563.18\n",
      "[123]\ttrain-rmse:130910+1438.18\ttest-rmse:174856+3461.67\n",
      "[124]\ttrain-rmse:130569+1467.75\ttest-rmse:174723+3516.26\n",
      "[125]\ttrain-rmse:130301+1528.29\ttest-rmse:174559+3480.65\n",
      "[126]\ttrain-rmse:129995+1514.3\ttest-rmse:174496+3554.01\n",
      "[127]\ttrain-rmse:129726+1478.35\ttest-rmse:174427+3629.32\n",
      "[128]\ttrain-rmse:129508+1515.89\ttest-rmse:174379+3624.47\n",
      "[129]\ttrain-rmse:129231+1562.43\ttest-rmse:174248+3591.73\n",
      "[130]\ttrain-rmse:128898+1562.68\ttest-rmse:174286+3487.03\n",
      "[131]\ttrain-rmse:128670+1576\ttest-rmse:174152+3455.45\n",
      "[132]\ttrain-rmse:128437+1569.06\ttest-rmse:174110+3508.86\n",
      "[133]\ttrain-rmse:128185+1607.08\ttest-rmse:174043+3541.8\n",
      "[134]\ttrain-rmse:128039+1622.33\ttest-rmse:173981+3532.45\n",
      "[135]\ttrain-rmse:127821+1585.59\ttest-rmse:173989+3462.42\n",
      "[136]\ttrain-rmse:127440+1505.76\ttest-rmse:173907+3469.26\n",
      "[137]\ttrain-rmse:127191+1449.86\ttest-rmse:173797+3391.97\n",
      "[138]\ttrain-rmse:126883+1359.89\ttest-rmse:173735+3487.63\n",
      "[139]\ttrain-rmse:126644+1427.62\ttest-rmse:173723+3447.23\n",
      "[140]\ttrain-rmse:126266+1440.85\ttest-rmse:173610+3364.21\n",
      "[141]\ttrain-rmse:126031+1445.03\ttest-rmse:173652+3499.29\n",
      "[142]\ttrain-rmse:125902+1458.43\ttest-rmse:173654+3503.56\n",
      "[143]\ttrain-rmse:125723+1394.95\ttest-rmse:173690+3572.93\n",
      "[144]\ttrain-rmse:125454+1416.56\ttest-rmse:173562+3605.6\n",
      "[145]\ttrain-rmse:125239+1442.39\ttest-rmse:173587+3583.68\n",
      "[146]\ttrain-rmse:125017+1504.02\ttest-rmse:173543+3564.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147]\ttrain-rmse:124748+1517.99\ttest-rmse:173430+3522.88\n",
      "[148]\ttrain-rmse:124547+1490.78\ttest-rmse:173337+3585.01\n",
      "[149]\ttrain-rmse:124265+1442.05\ttest-rmse:173197+3602.23\n",
      "[150]\ttrain-rmse:124048+1449.69\ttest-rmse:173167+3549.21\n",
      "[151]\ttrain-rmse:123889+1443.79\ttest-rmse:173196+3547.08\n",
      "[152]\ttrain-rmse:123744+1443.34\ttest-rmse:173245+3508.82\n",
      "[153]\ttrain-rmse:123500+1401.54\ttest-rmse:173096+3465.37\n",
      "[154]\ttrain-rmse:123243+1318.77\ttest-rmse:173036+3511.1\n",
      "[155]\ttrain-rmse:123040+1366.41\ttest-rmse:172993+3551.58\n",
      "[156]\ttrain-rmse:122853+1392.17\ttest-rmse:173026+3626.82\n",
      "[157]\ttrain-rmse:122623+1489.9\ttest-rmse:173017+3585.83\n",
      "[158]\ttrain-rmse:122356+1576.41\ttest-rmse:172905+3624.55\n",
      "[159]\ttrain-rmse:122107+1516.52\ttest-rmse:172771+3599.26\n",
      "[160]\ttrain-rmse:121813+1474.96\ttest-rmse:172618+3647.6\n",
      "[161]\ttrain-rmse:121619+1470.11\ttest-rmse:172541+3673.04\n",
      "[162]\ttrain-rmse:121455+1520.98\ttest-rmse:172578+3698.82\n",
      "[163]\ttrain-rmse:121217+1471.96\ttest-rmse:172447+3778.87\n",
      "[164]\ttrain-rmse:121065+1489.74\ttest-rmse:172438+3795.88\n",
      "[165]\ttrain-rmse:120839+1469.61\ttest-rmse:172368+3748.78\n",
      "[166]\ttrain-rmse:120653+1480.36\ttest-rmse:172302+3788.87\n",
      "[167]\ttrain-rmse:120446+1461.36\ttest-rmse:172170+3834.36\n",
      "[168]\ttrain-rmse:120220+1482.46\ttest-rmse:172123+3926.82\n",
      "[169]\ttrain-rmse:120061+1473.25\ttest-rmse:172152+3939.51\n",
      "[170]\ttrain-rmse:119863+1560.99\ttest-rmse:172200+3975.57\n",
      "[171]\ttrain-rmse:119698+1535.79\ttest-rmse:172198+3922.54\n",
      "[172]\ttrain-rmse:119424+1536.65\ttest-rmse:172086+3996.92\n",
      "[173]\ttrain-rmse:119220+1557.87\ttest-rmse:172087+3989.83\n",
      "[174]\ttrain-rmse:119047+1577.59\ttest-rmse:172013+3937.81\n",
      "[175]\ttrain-rmse:118902+1602.62\ttest-rmse:171974+3990.55\n",
      "[176]\ttrain-rmse:118662+1590.54\ttest-rmse:171974+3946.69\n",
      "[177]\ttrain-rmse:118561+1561.34\ttest-rmse:171991+3965.57\n",
      "[178]\ttrain-rmse:118344+1559.08\ttest-rmse:171897+4011.47\n",
      "[179]\ttrain-rmse:118207+1551.56\ttest-rmse:171874+4026.19\n",
      "[180]\ttrain-rmse:118029+1519.05\ttest-rmse:171854+4007.98\n",
      "[181]\ttrain-rmse:117887+1515.3\ttest-rmse:171887+3984.83\n",
      "[182]\ttrain-rmse:117711+1543.04\ttest-rmse:171889+3988.33\n",
      "[183]\ttrain-rmse:117501+1604.62\ttest-rmse:171846+3943.9\n",
      "[184]\ttrain-rmse:117254+1596.65\ttest-rmse:171695+4054.05\n",
      "[185]\ttrain-rmse:117119+1592.58\ttest-rmse:171710+4080.24\n",
      "[186]\ttrain-rmse:116933+1574.82\ttest-rmse:171683+4066.45\n",
      "[187]\ttrain-rmse:116691+1526.13\ttest-rmse:171654+4118.18\n",
      "[188]\ttrain-rmse:116556+1508.19\ttest-rmse:171645+4148.25\n",
      "[189]\ttrain-rmse:116366+1508.79\ttest-rmse:171665+4175.29\n",
      "[190]\ttrain-rmse:116138+1501.2\ttest-rmse:171613+4122.24\n",
      "[191]\ttrain-rmse:115930+1507.65\ttest-rmse:171543+4219.92\n",
      "[192]\ttrain-rmse:115743+1506.85\ttest-rmse:171484+4212.99\n",
      "[193]\ttrain-rmse:115516+1536.65\ttest-rmse:171487+4206.6\n",
      "[194]\ttrain-rmse:115358+1504.18\ttest-rmse:171548+4183.65\n",
      "[195]\ttrain-rmse:115223+1525.39\ttest-rmse:171489+4231.93\n",
      "[196]\ttrain-rmse:114983+1569.72\ttest-rmse:171435+4134.56\n",
      "[197]\ttrain-rmse:114824+1549.83\ttest-rmse:171398+4171.31\n",
      "[198]\ttrain-rmse:114666+1570.63\ttest-rmse:171350+4185.27\n",
      "[199]\ttrain-rmse:114533+1560.99\ttest-rmse:171326+4172\n",
      "[200]\ttrain-rmse:114407+1564.4\ttest-rmse:171371+4169.7\n",
      "[201]\ttrain-rmse:114221+1563.17\ttest-rmse:171299+4169.07\n",
      "[202]\ttrain-rmse:114027+1503.11\ttest-rmse:171233+4195.91\n",
      "[203]\ttrain-rmse:113915+1539.7\ttest-rmse:171223+4251.15\n",
      "[204]\ttrain-rmse:113759+1551.52\ttest-rmse:171197+4258.96\n",
      "[205]\ttrain-rmse:113597+1533.11\ttest-rmse:171199+4263.58\n",
      "[206]\ttrain-rmse:113431+1573.41\ttest-rmse:171154+4296.6\n",
      "[207]\ttrain-rmse:113283+1600.54\ttest-rmse:171095+4282.82\n",
      "[208]\ttrain-rmse:113173+1584.65\ttest-rmse:171048+4294.81\n",
      "[209]\ttrain-rmse:113019+1647.65\ttest-rmse:171046+4212.16\n",
      "[210]\ttrain-rmse:112861+1615.33\ttest-rmse:170961+4283.08\n",
      "[211]\ttrain-rmse:112670+1587.53\ttest-rmse:170880+4230.96\n",
      "[212]\ttrain-rmse:112486+1563.68\ttest-rmse:170847+4227.75\n",
      "[213]\ttrain-rmse:112358+1579.7\ttest-rmse:170784+4236.72\n",
      "[214]\ttrain-rmse:112208+1576.29\ttest-rmse:170682+4230.72\n",
      "[215]\ttrain-rmse:112073+1565.33\ttest-rmse:170671+4222.49\n",
      "[216]\ttrain-rmse:111911+1586.32\ttest-rmse:170627+4298.59\n",
      "[217]\ttrain-rmse:111798+1610.64\ttest-rmse:170679+4251.76\n",
      "[218]\ttrain-rmse:111625+1640.05\ttest-rmse:170604+4284.27\n",
      "[219]\ttrain-rmse:111503+1620.54\ttest-rmse:170602+4307.96\n",
      "[220]\ttrain-rmse:111382+1599.2\ttest-rmse:170609+4277.9\n",
      "[221]\ttrain-rmse:111225+1595.08\ttest-rmse:170548+4292.14\n",
      "[222]\ttrain-rmse:111013+1591.32\ttest-rmse:170500+4316.17\n",
      "[223]\ttrain-rmse:110881+1586.79\ttest-rmse:170463+4350.74\n",
      "[224]\ttrain-rmse:110753+1606.76\ttest-rmse:170471+4343.88\n",
      "[225]\ttrain-rmse:110622+1589.55\ttest-rmse:170465+4320.31\n",
      "[226]\ttrain-rmse:110471+1562.79\ttest-rmse:170416+4263.15\n",
      "[227]\ttrain-rmse:110323+1513.98\ttest-rmse:170392+4250.18\n",
      "[228]\ttrain-rmse:110175+1486.87\ttest-rmse:170377+4243.4\n",
      "[229]\ttrain-rmse:110065+1456.1\ttest-rmse:170436+4251.94\n",
      "[230]\ttrain-rmse:109983+1457.31\ttest-rmse:170411+4282.52\n",
      "[231]\ttrain-rmse:109910+1482.98\ttest-rmse:170425+4274.35\n",
      "[232]\ttrain-rmse:109744+1494.17\ttest-rmse:170343+4248.51\n",
      "[233]\ttrain-rmse:109536+1488.67\ttest-rmse:170304+4265.58\n",
      "[234]\ttrain-rmse:109398+1464.79\ttest-rmse:170242+4249.65\n",
      "[235]\ttrain-rmse:109236+1438.2\ttest-rmse:170191+4297.36\n",
      "[236]\ttrain-rmse:109115+1454.98\ttest-rmse:170157+4277.48\n",
      "[237]\ttrain-rmse:108990+1517.67\ttest-rmse:170136+4272.71\n",
      "[238]\ttrain-rmse:108815+1511.4\ttest-rmse:170070+4255.93\n",
      "[239]\ttrain-rmse:108680+1486.72\ttest-rmse:170109+4238.41\n",
      "[240]\ttrain-rmse:108524+1467.53\ttest-rmse:170106+4260.24\n",
      "[241]\ttrain-rmse:108425+1458.41\ttest-rmse:170103+4247.55\n",
      "[242]\ttrain-rmse:108240+1419.29\ttest-rmse:170059+4264.77\n",
      "[243]\ttrain-rmse:108042+1411.8\ttest-rmse:169942+4310.69\n",
      "[244]\ttrain-rmse:107830+1378.06\ttest-rmse:169896+4244.99\n",
      "[245]\ttrain-rmse:107673+1365.25\ttest-rmse:169842+4219.37\n",
      "[246]\ttrain-rmse:107520+1350.59\ttest-rmse:169827+4205.91\n",
      "[247]\ttrain-rmse:107403+1324.48\ttest-rmse:169847+4239.01\n",
      "[248]\ttrain-rmse:107319+1293.71\ttest-rmse:169875+4221.52\n",
      "[249]\ttrain-rmse:107190+1272.12\ttest-rmse:169870+4248.65\n",
      "[250]\ttrain-rmse:107093+1281.1\ttest-rmse:169853+4262.04\n",
      "[251]\ttrain-rmse:106924+1287\ttest-rmse:169821+4234.53\n",
      "[252]\ttrain-rmse:106820+1330.45\ttest-rmse:169813+4230.91\n",
      "[253]\ttrain-rmse:106730+1338.33\ttest-rmse:169792+4254.71\n",
      "[254]\ttrain-rmse:106641+1326.77\ttest-rmse:169761+4244.96\n",
      "[255]\ttrain-rmse:106536+1331.67\ttest-rmse:169763+4338.56\n",
      "[256]\ttrain-rmse:106452+1319.47\ttest-rmse:169728+4384.48\n",
      "[257]\ttrain-rmse:106337+1312.15\ttest-rmse:169719+4341.4\n",
      "[258]\ttrain-rmse:106192+1284.68\ttest-rmse:169744+4367.66\n",
      "[259]\ttrain-rmse:106088+1255.42\ttest-rmse:169728+4381.55\n",
      "[260]\ttrain-rmse:105978+1282.55\ttest-rmse:169717+4405.95\n",
      "[261]\ttrain-rmse:105840+1264.97\ttest-rmse:169682+4408.01\n",
      "[262]\ttrain-rmse:105715+1247.05\ttest-rmse:169651+4412.51\n",
      "[263]\ttrain-rmse:105522+1234.71\ttest-rmse:169655+4423.67\n",
      "[264]\ttrain-rmse:105375+1249.98\ttest-rmse:169659+4465.63\n",
      "[265]\ttrain-rmse:105283+1263.17\ttest-rmse:169643+4472.67\n",
      "[266]\ttrain-rmse:105175+1265.93\ttest-rmse:169686+4475.43\n",
      "[267]\ttrain-rmse:105024+1284.53\ttest-rmse:169667+4468.14\n",
      "[268]\ttrain-rmse:104907+1274.13\ttest-rmse:169659+4495.8\n",
      "[269]\ttrain-rmse:104785+1282.2\ttest-rmse:169648+4497.13\n",
      "[270]\ttrain-rmse:104670+1289.27\ttest-rmse:169632+4511.82\n",
      "[271]\ttrain-rmse:104534+1281.44\ttest-rmse:169635+4532.53\n",
      "[272]\ttrain-rmse:104439+1255.48\ttest-rmse:169643+4528.32\n",
      "[273]\ttrain-rmse:104323+1273.62\ttest-rmse:169625+4517.82\n",
      "[274]\ttrain-rmse:104194+1272.31\ttest-rmse:169590+4512.54\n",
      "[275]\ttrain-rmse:104033+1219.69\ttest-rmse:169577+4490.12\n",
      "[276]\ttrain-rmse:103903+1184.88\ttest-rmse:169558+4509.36\n",
      "[277]\ttrain-rmse:103808+1219.35\ttest-rmse:169541+4452.23\n",
      "[278]\ttrain-rmse:103711+1215.75\ttest-rmse:169500+4454.4\n",
      "[279]\ttrain-rmse:103609+1286.06\ttest-rmse:169514+4476.89\n",
      "[280]\ttrain-rmse:103474+1294.33\ttest-rmse:169510+4460.35\n",
      "[281]\ttrain-rmse:103392+1306.08\ttest-rmse:169523+4463.77\n",
      "[282]\ttrain-rmse:103281+1327.79\ttest-rmse:169454+4438.56\n",
      "[283]\ttrain-rmse:103175+1327.71\ttest-rmse:169429+4434.42\n",
      "[284]\ttrain-rmse:103064+1324.81\ttest-rmse:169425+4428.35\n",
      "[285]\ttrain-rmse:102957+1319.8\ttest-rmse:169378+4448.45\n",
      "[286]\ttrain-rmse:102858+1288.64\ttest-rmse:169371+4475.33\n",
      "[287]\ttrain-rmse:102723+1276.47\ttest-rmse:169365+4445.12\n",
      "[288]\ttrain-rmse:102593+1260.24\ttest-rmse:169344+4469.99\n",
      "[289]\ttrain-rmse:102468+1268.4\ttest-rmse:169269+4452.67\n",
      "[290]\ttrain-rmse:102353+1261.72\ttest-rmse:169264+4413.4\n",
      "[291]\ttrain-rmse:102207+1262.36\ttest-rmse:169248+4346.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[292]\ttrain-rmse:102085+1285.37\ttest-rmse:169273+4361.63\n",
      "[293]\ttrain-rmse:101986+1312.31\ttest-rmse:169253+4335.8\n",
      "[294]\ttrain-rmse:101853+1315.7\ttest-rmse:169181+4301.36\n",
      "[295]\ttrain-rmse:101769+1325.63\ttest-rmse:169164+4285.31\n",
      "[296]\ttrain-rmse:101667+1313.66\ttest-rmse:169163+4321\n",
      "[297]\ttrain-rmse:101547+1299.8\ttest-rmse:169185+4309.02\n",
      "[298]\ttrain-rmse:101415+1276.39\ttest-rmse:169181+4338.55\n",
      "[299]\ttrain-rmse:101327+1256.27\ttest-rmse:169190+4370.4\n",
      "[300]\ttrain-rmse:101227+1228.78\ttest-rmse:169177+4367.2\n",
      "[301]\ttrain-rmse:101138+1215.6\ttest-rmse:169140+4361.91\n",
      "[302]\ttrain-rmse:101031+1237.7\ttest-rmse:169173+4405.82\n",
      "[303]\ttrain-rmse:100884+1204.19\ttest-rmse:169152+4394.35\n",
      "[304]\ttrain-rmse:100789+1209.34\ttest-rmse:169150+4421.27\n",
      "[305]\ttrain-rmse:100707+1214.85\ttest-rmse:169096+4465.33\n",
      "[306]\ttrain-rmse:100618+1188.83\ttest-rmse:169101+4482.41\n",
      "[307]\ttrain-rmse:100539+1164.82\ttest-rmse:169064+4525.17\n",
      "[308]\ttrain-rmse:100437+1148.78\ttest-rmse:169074+4497.71\n",
      "[309]\ttrain-rmse:100351+1163.66\ttest-rmse:169066+4496.18\n",
      "[310]\ttrain-rmse:100232+1144.07\ttest-rmse:169074+4483.86\n",
      "[311]\ttrain-rmse:100138+1120.08\ttest-rmse:169077+4459.53\n",
      "[312]\ttrain-rmse:100051+1132.59\ttest-rmse:169091+4467.47\n",
      "[313]\ttrain-rmse:99948.5+1127.34\ttest-rmse:169106+4468.39\n",
      "[314]\ttrain-rmse:99820.7+1131.46\ttest-rmse:169073+4454.22\n",
      "[315]\ttrain-rmse:99681.5+1102.04\ttest-rmse:169058+4452.23\n",
      "[316]\ttrain-rmse:99589+1110.77\ttest-rmse:169056+4405.98\n",
      "[317]\ttrain-rmse:99494.9+1110.96\ttest-rmse:169022+4405.99\n",
      "[318]\ttrain-rmse:99404.8+1134.53\ttest-rmse:169046+4404\n",
      "[319]\ttrain-rmse:99273.3+1115.44\ttest-rmse:169062+4391.56\n",
      "[320]\ttrain-rmse:99199.8+1103.55\ttest-rmse:169092+4403.37\n",
      "[321]\ttrain-rmse:99069+1100.39\ttest-rmse:169080+4445.6\n",
      "[322]\ttrain-rmse:98977.8+1106.81\ttest-rmse:169072+4444.9\n",
      "[323]\ttrain-rmse:98908.5+1098.65\ttest-rmse:169073+4501.46\n",
      "[324]\ttrain-rmse:98807.1+1099.26\ttest-rmse:169056+4512.68\n",
      "[325]\ttrain-rmse:98711.4+1087.95\ttest-rmse:169053+4493.44\n",
      "[326]\ttrain-rmse:98624.8+1072.71\ttest-rmse:169069+4481.76\n",
      "[327]\ttrain-rmse:98519.6+1091.04\ttest-rmse:169084+4467.07\n",
      "[328]\ttrain-rmse:98433.8+1092.15\ttest-rmse:169050+4453.41\n",
      "[329]\ttrain-rmse:98321.8+1068.58\ttest-rmse:169059+4420.47\n",
      "[330]\ttrain-rmse:98220.2+1063.57\ttest-rmse:169050+4428.11\n",
      "[331]\ttrain-rmse:98125.7+1092.47\ttest-rmse:169020+4411.54\n",
      "[332]\ttrain-rmse:98006.4+1093.59\ttest-rmse:169014+4443.11\n",
      "[333]\ttrain-rmse:97905.7+1079.05\ttest-rmse:169008+4466.05\n",
      "[334]\ttrain-rmse:97811.1+1078.49\ttest-rmse:168981+4476.91\n",
      "[335]\ttrain-rmse:97729.3+1080.71\ttest-rmse:168978+4463.18\n",
      "[336]\ttrain-rmse:97626.4+1057.18\ttest-rmse:168979+4444.9\n",
      "[337]\ttrain-rmse:97543.3+1017.18\ttest-rmse:168981+4466.35\n",
      "[338]\ttrain-rmse:97443.6+995.373\ttest-rmse:168983+4476.14\n",
      "[339]\ttrain-rmse:97316.9+980.998\ttest-rmse:168965+4439.05\n",
      "[340]\ttrain-rmse:97245.8+999.961\ttest-rmse:168973+4431.18\n",
      "[341]\ttrain-rmse:97151.1+983.823\ttest-rmse:168964+4474.04\n",
      "[342]\ttrain-rmse:97064.7+970.397\ttest-rmse:168961+4484.53\n",
      "[343]\ttrain-rmse:96982.3+986.422\ttest-rmse:168981+4495.26\n",
      "[344]\ttrain-rmse:96880.8+989.42\ttest-rmse:169002+4486.72\n",
      "[345]\ttrain-rmse:96764.5+998.833\ttest-rmse:168985+4483.9\n",
      "[346]\ttrain-rmse:96674.3+1018.8\ttest-rmse:168996+4468.88\n",
      "[347]\ttrain-rmse:96566.2+1009.08\ttest-rmse:169041+4473.72\n",
      "[348]\ttrain-rmse:96476.7+1025.17\ttest-rmse:169007+4444.59\n",
      "[349]\ttrain-rmse:96371.3+1009.35\ttest-rmse:168966+4439.3\n",
      "[350]\ttrain-rmse:96302.3+982.902\ttest-rmse:168972+4452.83\n",
      "[351]\ttrain-rmse:96181.2+948.01\ttest-rmse:168950+4453.76\n",
      "[352]\ttrain-rmse:96093.4+948.066\ttest-rmse:168970+4467.98\n",
      "[353]\ttrain-rmse:96015.8+928.659\ttest-rmse:168973+4462.36\n",
      "[354]\ttrain-rmse:95911.9+913.036\ttest-rmse:168949+4426.97\n",
      "[355]\ttrain-rmse:95829.3+933.212\ttest-rmse:168962+4421.03\n",
      "[356]\ttrain-rmse:95735+938.771\ttest-rmse:168969+4441.68\n",
      "[357]\ttrain-rmse:95648.7+940.557\ttest-rmse:168924+4441.67\n",
      "[358]\ttrain-rmse:95590+930.869\ttest-rmse:168914+4440.9\n",
      "[359]\ttrain-rmse:95510.6+921.013\ttest-rmse:168862+4455.52\n",
      "[360]\ttrain-rmse:95418.2+886.804\ttest-rmse:168851+4483.16\n",
      "[361]\ttrain-rmse:95336.3+876.817\ttest-rmse:168863+4488.3\n",
      "[362]\ttrain-rmse:95243.8+848.385\ttest-rmse:168874+4479\n",
      "[363]\ttrain-rmse:95158.9+842.538\ttest-rmse:168836+4470.8\n",
      "[364]\ttrain-rmse:95064.6+845.489\ttest-rmse:168819+4390.05\n",
      "[365]\ttrain-rmse:94983.5+826.656\ttest-rmse:168814+4376.79\n",
      "[366]\ttrain-rmse:94867.6+799.663\ttest-rmse:168791+4362.66\n",
      "[367]\ttrain-rmse:94781.1+824.746\ttest-rmse:168791+4376.52\n",
      "[368]\ttrain-rmse:94688.3+831.341\ttest-rmse:168767+4401.45\n",
      "[369]\ttrain-rmse:94602.5+852.38\ttest-rmse:168783+4393.6\n",
      "[370]\ttrain-rmse:94514.1+845.066\ttest-rmse:168761+4349.12\n",
      "[371]\ttrain-rmse:94433.9+833.302\ttest-rmse:168751+4379.45\n",
      "[372]\ttrain-rmse:94354+864.618\ttest-rmse:168789+4375.04\n",
      "[373]\ttrain-rmse:94248.9+857.61\ttest-rmse:168795+4370.93\n",
      "[374]\ttrain-rmse:94160.8+860.985\ttest-rmse:168794+4354.86\n",
      "[375]\ttrain-rmse:94061.7+866.325\ttest-rmse:168779+4366.98\n",
      "[376]\ttrain-rmse:93985.7+860.361\ttest-rmse:168773+4373.86\n",
      "[377]\ttrain-rmse:93900.7+854.333\ttest-rmse:168776+4380.01\n",
      "[378]\ttrain-rmse:93807.2+861.925\ttest-rmse:168766+4386.62\n",
      "[379]\ttrain-rmse:93726.1+866.721\ttest-rmse:168798+4395.1\n",
      "[380]\ttrain-rmse:93638+898.137\ttest-rmse:168796+4394.09\n",
      "[381]\ttrain-rmse:93547.9+879.389\ttest-rmse:168795+4380.84\n",
      "[382]\ttrain-rmse:93483.1+881.768\ttest-rmse:168794+4391.89\n",
      "[383]\ttrain-rmse:93407.7+885.365\ttest-rmse:168812+4406.34\n",
      "[384]\ttrain-rmse:93316+886.572\ttest-rmse:168775+4380.39\n",
      "[385]\ttrain-rmse:93253.2+904.373\ttest-rmse:168763+4376.44\n",
      "[386]\ttrain-rmse:93134.9+885.926\ttest-rmse:168764+4363.48\n",
      "[387]\ttrain-rmse:93043.5+860.214\ttest-rmse:168773+4393.04\n",
      "[388]\ttrain-rmse:92968.6+849.743\ttest-rmse:168773+4386.41\n",
      "[389]\ttrain-rmse:92881+864.414\ttest-rmse:168775+4391.05\n",
      "[390]\ttrain-rmse:92801.5+903.121\ttest-rmse:168757+4365.65\n",
      "[391]\ttrain-rmse:92725.4+889.232\ttest-rmse:168763+4364.01\n",
      "[392]\ttrain-rmse:92635.7+900.816\ttest-rmse:168787+4363.07\n",
      "[393]\ttrain-rmse:92556.1+880.108\ttest-rmse:168750+4344.45\n",
      "[394]\ttrain-rmse:92470+877.87\ttest-rmse:168725+4369.8\n",
      "[395]\ttrain-rmse:92394.4+862.204\ttest-rmse:168696+4425.89\n",
      "[396]\ttrain-rmse:92304.1+833.246\ttest-rmse:168706+4464.36\n",
      "[397]\ttrain-rmse:92241.7+834.44\ttest-rmse:168662+4443.21\n",
      "[398]\ttrain-rmse:92174.7+833.333\ttest-rmse:168664+4421.6\n",
      "[399]\ttrain-rmse:92100.9+819.67\ttest-rmse:168652+4411.12\n",
      "[400]\ttrain-rmse:92003.9+805.015\ttest-rmse:168645+4433.99\n",
      "[401]\ttrain-rmse:91931.5+786.142\ttest-rmse:168698+4411.16\n",
      "[402]\ttrain-rmse:91846.6+764.528\ttest-rmse:168671+4421.17\n",
      "[403]\ttrain-rmse:91745.6+780.513\ttest-rmse:168678+4443.26\n",
      "[404]\ttrain-rmse:91651.6+749.878\ttest-rmse:168672+4431.29\n",
      "[405]\ttrain-rmse:91592+740.656\ttest-rmse:168705+4429.15\n",
      "\n",
      "Model Report\n",
      "Mean Square error : 9.308e+09\n",
      "r2 Score : 0.9333\n"
     ]
    }
   ],
   "source": [
    "xgb2 = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=406,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.7,\n",
    " colsample_bytree=0.9,\n",
    " objective= 'reg:linear',\n",
    " reg_alpha = 100, \n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=100)\n",
    "\n",
    "m2pred = modelfit(xgb2, df_train, predictors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even though the train-rmse is slightly higher, the test-rmse is now lower. However, the coefficient of determination (r2 score) decreased. We will check the model performance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg difference between prediction and true price (training set):  52.71912695896736\n",
      "Avg difference between prediction and true price (test set):  -661.6901460546283\n"
     ]
    }
   ],
   "source": [
    "# try to predict log error based on user input \n",
    "xgb2.fit(df_train[predictors], df_train['price'],eval_metric='rmse')\n",
    "        \n",
    "#Predict training set:\n",
    "predictions = xgb2.predict(df_train[predictors])\n",
    "df_train['pred'] = predictions\n",
    "\n",
    "#combine x_test and y_test for model evaluation \n",
    "df_test = pd.concat([x_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
    "test_pred = xgb2.predict(df_test[predictors])\n",
    "df_test['pred'] = test_pred\n",
    "\n",
    "df_train['pred - price'] = df_train['pred'] - df_train['price']\n",
    "df_test['pred - price'] = df_test['pred'] - df_test['price']\n",
    "\n",
    "print('')\n",
    "print('Avg difference between prediction and true price (training set): ', df_train['pred - price'].mean())\n",
    "print('Avg difference between prediction and true price (test set): ', df_test['pred - price'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the feature importance, we can tell that the most important features are fullbathcnt, propertylandusetypeid, regionidcounty, and bedroomcnt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      features  feature importance\n",
      "2  sqft_living  0.248148          \n",
      "4  zipcode      0.220763          \n",
      "3  sqft_lot     0.207446          \n",
      "5  year         0.138329          \n",
      "1  bathrooms    0.111132          \n",
      "0  bedrooms     0.074182          \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x222a6b72518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot feature importance manually\n",
    "from numpy import loadtxt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "fi = pd.DataFrame()\n",
    "fi['features'] = df_test.columns[1:-3]\n",
    "fi['feature importance'] = xgb2.feature_importances_\n",
    "#fi = fi.sort(['feature importance'], ascending = [0])\n",
    "#fi = pd.DataFrame(fi)\n",
    "fi = fi.sort_values(['feature importance'], ascending=[0])\n",
    "print(fi)\n",
    "\n",
    "# plot\n",
    "fi.plot(kind='bar', x='features')\n",
    "plt.title('Figure 3: Feature Importance', size = 20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id  bedrooms  bathrooms  sqft_living  sqft_lot  zipcode  year  \\\n",
      "0      3990200065  4         2.50       2050         9143      98166    1992   \n",
      "1      1525069021  3         2.50       2580         214315    98053    1986   \n",
      "2      9553200125  3         1.50       2440         5750      98115    1939   \n",
      "3      3080000030  3         2.50       2230         4000      98144    1954   \n",
      "4      3782760040  3         3.25       2780         4002      98019    2009   \n",
      "5      5152960350  4         2.75       2390         9650      98003    1976   \n",
      "6      3992700585  3         1.75       1880         9360      98125    1941   \n",
      "7      6071700020  3         2.25       1640         8400      98006    1962   \n",
      "8      3902310210  4         2.50       2100         8800      98033    1980   \n",
      "9      4435000705  3         1.00       1350         8700      98188    1942   \n",
      "10     925049278   4         2.00       1490         4054      98115    1926   \n",
      "11     9834200165  4         1.50       1790         4080      98144    1928   \n",
      "12     7853321180  5         2.50       2550         6405      98065    2008   \n",
      "13     2473400290  4         2.50       1870         8190      98058    1977   \n",
      "14     3293700480  4         1.75       2200         8545      98133    1982   \n",
      "15     5104512070  4         3.00       2430         7242      98038    2003   \n",
      "16     9274200316  3         2.50       1680         934       98116    2008   \n",
      "17     1392800035  2         1.00       1240         6400      98126    1938   \n",
      "18     6791100410  3         2.50       1660         15000     98075    1970   \n",
      "19     4139480200  4         3.25       4290         12103     98006    1997   \n",
      "20     2314300200  4         3.00       2580         7299      98058    1998   \n",
      "21     8731730710  3         1.00       1180         9000      98031    1970   \n",
      "22     2193310320  4         2.50       2330         7064      98052    1984   \n",
      "23     3052700610  6         3.50       2820         5400      98117    1958   \n",
      "24     427000065   5         2.50       4340         9108      98118    1979   \n",
      "25     1450900020  3         2.00       1610         8416      98031    1994   \n",
      "26     2473250280  4         2.25       2300         9100      98058    1977   \n",
      "27     6821102367  3         2.50       1570         1452      98199    2007   \n",
      "28     5315100394  3         1.00       1440         13824     98040    1957   \n",
      "29     4217400420  3         1.50       1340         6000      98105    1927   \n",
      "...           ... ..          ...        ...          ...        ...     ...   \n",
      "15348  2770601595  2         1.00       1470         6000      98199    1944   \n",
      "15349  1126059201  5         3.25       4410         35192     98072    1990   \n",
      "15350  1954440060  3         2.50       1900         8744      98074    1987   \n",
      "15351  985001266   3         1.50       2210         11111     98168    1934   \n",
      "15352  8901000543  3         2.50       2590         7237      98125    2004   \n",
      "15353  2491200330  3         2.50       1690         5131      98126    1998   \n",
      "15354  5101405604  1         1.00       900          6380      98125    1947   \n",
      "15355  3885805640  3         1.50       1300         7200      98033    1960   \n",
      "15356  1423069162  4         2.25       2740         88426     98027    1991   \n",
      "15357  8669160270  3         2.50       1550         3402      98002    2009   \n",
      "15358  7334501300  3         1.75       1630         11475     98045    1979   \n",
      "15359  8122100595  2         1.00       940          5040      98126    1926   \n",
      "15360  2487200938  5         3.25       3230         5000      98136    2002   \n",
      "15361  9165100130  3         1.75       1180         4080      98117    1928   \n",
      "15362  2413300240  4         2.25       1990         7350      98003    1978   \n",
      "15363  9273200140  2         2.25       3950         3938      98116    1991   \n",
      "15364  3649100473  3         1.50       1300         12240     98028    1963   \n",
      "15365  6908200650  3         2.50       2330         1987      98107    2004   \n",
      "15366  2545900050  3         1.00       1360         9948      98010    1977   \n",
      "15367  6683000295  3         2.50       2010         14298     98070    1977   \n",
      "15368  2517010230  3         2.50       1800         3980      98042    2006   \n",
      "15369  2024059058  4         2.75       2890         7821      98006    2014   \n",
      "15370  1133000235  6         2.25       3550         11780     98125    1948   \n",
      "15371  2321300351  2         1.00       1510         4032      98199    1935   \n",
      "15372  1924069105  3         1.75       3150         9258      98027    1970   \n",
      "15373  59000445    4         2.75       2240         5400      98116    1940   \n",
      "15374  8669160170  3         2.50       1550         3569      98002    2011   \n",
      "15375  1862000010  4         2.50       3400         35062     98052    1988   \n",
      "15376  108000127   4         3.50       2000         2309      98177    2008   \n",
      "15377  8931100095  2         2.25       2130         5920      98115    1950   \n",
      "\n",
      "           price          pred   pred - price  \n",
      "0      360000.0   4.049185e+05  44918.531250   \n",
      "1      400000.0   6.915698e+05  291569.750000  \n",
      "2      875000.0   7.569297e+05 -118070.312500  \n",
      "3      398750.0   5.728254e+05  174075.375000  \n",
      "4      402500.0   5.171340e+05  114634.000000  \n",
      "5      379750.0   3.516691e+05 -28080.937500   \n",
      "6      445500.0   4.887148e+05  43214.750000   \n",
      "7      515000.0   5.036038e+05 -11396.218750   \n",
      "8      610000.0   6.129696e+05  2969.562500    \n",
      "9      160000.0   2.257884e+05  65788.359375   \n",
      "10     607000.0   5.467052e+05 -60294.750000   \n",
      "11     704300.0   5.475206e+05 -156779.375000  \n",
      "12     465000.0   4.763438e+05  11343.750000   \n",
      "13     285000.0   3.100843e+05  25084.281250   \n",
      "14     414950.0   4.599553e+05  45005.281250   \n",
      "15     412000.0   3.896428e+05 -22357.156250   \n",
      "16     558000.0   4.961596e+05 -61840.375000   \n",
      "17     559000.0   3.831919e+05 -175808.125000  \n",
      "18     432000.0   5.073375e+05  75337.500000   \n",
      "19     1400000.0  1.413847e+06  13847.250000   \n",
      "20     449500.0   4.370232e+05 -12476.781250   \n",
      "21     215000.0   2.012601e+05 -13739.875000   \n",
      "22     595000.0   6.174056e+05  22405.562500   \n",
      "23     850000.0   6.974809e+05 -152519.062500  \n",
      "24     537500.0   6.975774e+05  160077.375000  \n",
      "25     268000.0   2.636666e+05 -4333.406250    \n",
      "26     265000.0   3.842643e+05  119264.343750  \n",
      "27     547000.0   4.796681e+05 -67331.875000   \n",
      "28     604000.0   6.363005e+05  32300.500000   \n",
      "29     907000.0   5.915524e+05 -315447.562500  \n",
      "...         ...            ...            ...  \n",
      "15348  415000.0   5.231148e+05  108114.812500  \n",
      "15349  1270000.0  1.163281e+06 -106719.000000  \n",
      "15350  560000.0   5.080790e+05 -51921.031250   \n",
      "15351  250000.0   3.230837e+05  73083.718750   \n",
      "15352  620000.0   6.791139e+05  59113.937500   \n",
      "15353  460000.0   4.332813e+05 -26718.718750   \n",
      "15354  350000.0   3.417115e+05 -8288.468750    \n",
      "15355  625000.0   5.062425e+05 -118757.500000  \n",
      "15356  549000.0   5.901576e+05  41157.562500   \n",
      "15357  273500.0   2.197576e+05 -53742.437500   \n",
      "15358  308000.0   3.447756e+05  36775.625000   \n",
      "15359  212700.0   3.746257e+05  161925.656250  \n",
      "15360  815000.0   8.554246e+05  40424.562500   \n",
      "15361  450000.0   5.327409e+05  82740.875000   \n",
      "15362  280000.0   2.667360e+05 -13264.031250   \n",
      "15363  1310000.0  1.309806e+06 -194.125000     \n",
      "15364  365000.0   3.407948e+05 -24205.156250   \n",
      "15365  732000.0   7.519399e+05  19939.875000   \n",
      "15366  234950.0   2.915794e+05  56629.375000   \n",
      "15367  350000.0   4.340433e+05  84043.312500   \n",
      "15368  286000.0   2.735853e+05 -12414.687500   \n",
      "15369  978000.0   9.245812e+05 -53418.812500   \n",
      "15370  450000.0   5.994327e+05  149432.687500  \n",
      "15371  575000.0   5.670426e+05 -7957.375000    \n",
      "15372  450000.0   5.931333e+05  143133.312500  \n",
      "15373  590000.0   6.721747e+05  82174.687500   \n",
      "15374  259000.0   2.303534e+05 -28646.562500   \n",
      "15375  915000.0   8.611158e+05 -53884.250000   \n",
      "15376  456500.0   4.326678e+05 -23832.218750   \n",
      "15377  779000.0   7.234751e+05 -55524.937500   \n",
      "\n",
      "[15378 rows x 10 columns]\n",
      "r2 Score : 0.7858\n"
     ]
    }
   ],
   "source": [
    "# final model \n",
    "\n",
    "xgb2 = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=406,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.7,\n",
    " colsample_bytree=0.9,\n",
    " objective= 'reg:linear',\n",
    " reg_alpha = 100, \n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=100)\n",
    "\n",
    "# data set used for training\n",
    "print(df_train)\n",
    "xgb2.fit(df_train[predictors], df_train['price'],eval_metric='rmse')\n",
    "dtest_predictions = xgb2.predict(df_test[predictors])\n",
    "print(\"r2 Score : %.4g\" % metrics.r2_score(df_test['price'].values, dtest_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
